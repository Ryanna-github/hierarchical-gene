\documentclass[12pt, a4paper, oneside]{article}
\usepackage{amsmath, amsthm, amssymb, bm, graphicx, hyperref, mathrsfs}
\usepackage{geometry}
\usepackage{amsmath}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage[ruled, lined, linesnumbered, commentsnumbered, longend]{algorithm2e}
\usepackage{booktabs}
\usepackage{breqn}
\usepackage{natbib}
\usepackage{amsmath}
\numberwithin{equation}{section}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage[UTF8]{ctex}
\usepackage{subfigure}

\title{\textbf{Hierarchical heterogeneous analysis of high-dimensional data
		based on interactions}}
\author{Yan Ren \\ 2021103739}
\date{\today}
\linespread{1.5}
\newcounter{problemname}
\newenvironment{problem}{\stepcounter{problemname}\par\noindent\textsc{Problem \arabic{problemname}. }}{\\\par}
\newenvironment{solution}{\par\noindent\textsc{Solution. }}{\\\par}
\newenvironment{note}{\par\noindent\textsc{Note of Problem \arabic{problemname}. }}{\\\par}
\newcommand{\nysm}{Nystr$\ddot{\rm o}$m Method}
\geometry{a4paper, scale = 0.8}

\begin{document}
	
\maketitle
\newpage
\tableofcontents
\newpage

\section{Problem} % =====================================================
\label{sec:problem}

Consider a supervised group regression problem. There are $n$ observations and corresponding response values. Denote $y_i$ as the $i^{th}$ response. Suppose two types of features $X_i$ and $Z_i$ for each subject, where $X_i \in \mathbb{R}^{p}$ and $Z_i \in \mathbb{R}^{q}$. We aim to explore the subgroup structure of data. 

The hierarchical model structure is adopted, where both main and interaction effects are considered and combined.

\section{Model} % =====================================================
\label{sec:model}

\begin{itemize}
	\item $X = (X_1,...,X_n)^{T} \in \mathbb{R}^{n\times p}$, type \uppercase\expandafter{\romannumeral1} features (main G) for all observations, where $X_i \in \mathbb{R}^{p}$;
	\item $Z = (Z_1,...,Z_n)^{T} \in \mathbb{R}^{n\times q}$, type \uppercase\expandafter{\romannumeral2} features (main E) for all observations, where $Z_i \in \mathbb{R}^{q}$;
	\item $y = (y_1,...,y_n)^{T} \in \mathbb{R}^{n}$, the response vector.
\end{itemize}


\subsection{Object Function}
\label{subsec:object-function}

Consider the heterogeneity model:

\begin{equation}
	\label{eq:model1}
	y_{i}=X_{i}^{\top} \beta_{i}+Z_{i}^{\top} \alpha_{i}+\sum_{s=1}^{q} W_{i}^{(s)} \eta_{i s}+\varepsilon_{i}.
\end{equation}
with the assumption that $\epsilon_i \sim^{(iid)} N(0, \sigma^2)$,
where
\begin{equation}
	\left\{
	\begin{aligned}
		&W_{i}^{(s)}=\left(Z_{i s} X_{i 1}, \ldots, Z_{i s} X_{i p}\right)\in \mathbb{R}^{p}, \\ &\beta_{i}=\left(\beta_{i 1}, \ldots, \beta_{i p}\right)^{\top}\in \mathbb{R}^{p}, \\ &\alpha_{i}=\left(\alpha_{i 1}, \ldots, \alpha_{i p}\right)^{\top}\in \mathbb{R}^{q} \\
		&\eta_{i s}=\left(\eta_{i s 1}, \ldots, \eta_{i s p}\right)^{\top}\in \mathbb{R}^{p} \\
	\end{aligned}
	\right.
	\nonumber
\end{equation}

$W_{i} = (W_{i}^{(1)},...,W_{i}^{(q)})^{T}$ is the interaction variable composed of features from both $X_i$ and $Z_i$. $\beta_i,\ \alpha_i,\eta_{is}$ are corresponding coefficients. 

Especially, coefficient for interactions can be deomposed as 
\begin{equation}
	\begin{aligned}
		\eta_{isj} &= \beta_{ij}\gamma_{isj} \\
		\Rightarrow \eta_{is} &= \beta_i \odot \gamma_{is} = (\beta_{i1}\gamma_{is_1},...,\beta_{ip}\gamma_{is_p}) ^{T}
	\end{aligned}
\end{equation}

($\odot$: component-wise product. $\otimes$: Kronecker product). 

In this way, interaction coefficient can be understood as main effect from $X$ and interaction parameters between $X$ and $Z$. Now, three sets of independent coefficients are to be calculated: $\beta, \alpha, \gamma$.

To make it clear, the model is written as equation \ref{eq:model-matrix}

\begin{equation}
	\label{eq:model-matrix}
	\begin{aligned}
		y_i &= X_i^T\beta_i + Z_i^T\alpha_i + W_i^{T}(\beta_i \otimes I_{q\times 1}\odot \gamma_i) + \epsilon_i \\
		&= X_i^T\beta_i + Z_i^T\alpha_i + W_i^{T}\eta_i + \epsilon_i;
	\end{aligned}
\end{equation}
\\
Denote $y = (y_1, ..., y_n)^T\in \mathbb{R}^n$, the full model in matrix form is:
\begin{equation}
	\label{eq:model-matrix-full}
	y = X^T\beta + Z^{T}\alpha + W^T(\beta \otimes I_{q\times 1}\odot \gamma) + \epsilon
\end{equation}

\vspace{0.4cm}where $X = \begin{bmatrix} X_1^T &  &  \\  & \ddots & \\  & & X_n^T \end{bmatrix}_{n\times np}$, $Z = \begin{bmatrix} Z_1^T &  &  \\  & \ddots & \\  & & Z_n^T \end{bmatrix}_{n\times nq}$, $W = \begin{bmatrix} W_1^T &  &  \\  & \ddots & \\  & & W_n^T \end{bmatrix}_{n\times npq}$

\vspace{0.4cm}\ \ \ \qquad$\beta = \begin{bmatrix} \beta_1 \\ \vdots \\ \beta_n \end{bmatrix}_{np\times 1}$, $\alpha = \begin{bmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{bmatrix}_{nq\times 1}$, $\gamma_i = \begin{bmatrix} \gamma_{i1} \\ \vdots \\ \gamma_{iq} \end{bmatrix}_{pq\times 1}\Rightarrow\gamma = \begin{bmatrix} \gamma_{1} \\ \vdots \\ \gamma_{n} \end{bmatrix}_{npq\times 1}$

%$\beta \otimes I_{q\times 1}\odot \gamma = \begin{bmatrix} \beta_1\otimes I_{q\times 1}\odot \gamma_1 \\ \vdots \\ \beta_n\otimes I_{q\times 1}\odot \gamma_n \end{bmatrix}_{npq\times 1}$

\vspace{0.5cm}
We consider a hierarchical subgroup structure. Specifically, $\beta_{i}$ and $\alpha_i$ define a first-level heterogeneity structure with $K_{1}$ subgroups, and $\gamma_{i}$ define a second-level heterogeneity structure with $K_{2}$ subgroups. Each first-level subgroup is a union of one or several subgroups in the second level. Denote $\left\{\mathcal{G}_{1}, \ldots, \mathcal{G}_{K_{1}}\right\}$ as the collection of subject index sets of the $K_{1}$ first-level subgroups, and $\left\{\mathcal{T}_{1}, \ldots, \mathcal{T}_{K_{2}}\right\}$ as the collection of subject index sets of the $K_{2}$ second-level subgroups. Then there exists a partition of $\left\{1, \ldots, K_{2}\right\}:\left\{\mathcal{H}_{1}, \ldots, \mathcal{H}_{K_{1}}\right\}$ satisfying $\mathcal{G}_{k_{1}}=\bigcup_{k_{2} \in \mathcal{H}_{k_{1}}} \mathcal{T}_{k_{2}}$ for $1 \leq k_{1} \leq K_{1}$ and $1 \leq k_{2} \leq K_{2}$. The subjects within $k_{1}$ th first-level subgroup have the identical coefficients of main $\mathrm{E}$ and $\mathrm{G}$, denoted as $B_{k_1} = (\beta^T, \alpha^T)^T_{k_1}$, and those within $k_{2}$ th second-level subgroup have the identical coefficients of interactions, denoted as $\gamma_{k_{2}}$. Overall, the response variable $y_{i}$ 's satisfy the following distribution:
\begin{equation}
\begin{aligned}
	f\left(y_{i} \mid X_{i}\right) &=\sum_{k_{1}=1}^{K_{1}} \pi_{k_{1}} \sum_{k_{2} \in \mathcal{H}_{k_{1}}} \frac{\pi_{k_{2}}}{\pi_{k_{1}}} f\left(y_{i} \mid X_{i}, B_{k_{1}}, \gamma_{k_{2}}\right) \\
	&=\sum_{k_{2}=1}^{K_{2}} \pi_{k_{2}} f\left(y_{i} \mid X_{i}, B_{\mathcal{F}\left(k_{2}\right)}, \gamma_{k_{2}}\right)
\end{aligned}
\end{equation}
where $\pi_{k_{1}}$ and $\pi_{k_{2}}$ are unknown mixture probability of first- and second-level subgroups respectively, and $\mathcal{F}(\cdot)$ is a mapping from second-level subgroup index sets to first-level subgroup index sets, i.e., $\mathcal{F}\left(k_{2}\right)=k_{1}$ for $k_{2} \in \mathcal{H}_{k_{1}}$.

\subsection{Penalized estimation}
\label{subsec:penalty}

For model \ref{eq:model-matrix-full}, we assume that each observation corresponds to a set of parameters $(\beta_i, \alpha_i, \gamma_i), i =1,2,...,n$. However, samples of the same (subsub)groups should share the same set of parameters (samples from the same subgroup share $(\beta, \alpha)$, while those from the same subsubgroup share $(\beta, \alpha, \gamma)$). Denote parameters for groups as $\{(\beta_k, \alpha_k, \gamma_k), k = 1, 2, ..., K\}$ where $K$ means subsubgroup number. From now on, the subscript indicates group index instead of sample index.

Now we propose the penalized objective functions (Since we use Gaussian mixture model, $f_k$ is gaussian density function for group $k$):
\begin{equation}
	\label{eq:obj1}
	\begin{aligned}
		\mathcal{L}(\beta, \alpha, \gamma, \pi \mid X, y)
		&=-\sum_{i=1}^{n} \log \sum_{k=1}^{K} \pi_{k} f_{k}\left(y_{i} \mid X_{i}, \beta_k, \alpha_k, \gamma_k\right)+pen(\beta, \alpha, \gamma)\\
		&=-\sum_{i=1}^{n} \log \sum_{k=1}^{K} \pi_{k} 
		\frac{1}{\sqrt{2\pi} \sigma} \exp\{-\frac{1}{2\sigma^2}\left[(y_i - \mu_k)^2\right]\} \\
		&\qquad+pen(\beta, \alpha, \gamma)
	\end{aligned}
\end{equation}
where
\begin{equation}
	\begin{aligned}
		\begin{aligned}
			\operatorname{pen}(\beta, \alpha, \gamma)
			&=\sum_{k=1}^{K} \sum_{j=1}^{p} \operatorname{pen}\left(\left|\beta_{k j}\right|, \lambda_{1}\right)+\sum_{k=1}^{K} \sum_{s=1}^{q} \sum_{j=1}^{p} \operatorname{pen}\left(\left|\gamma_{k s j}\right|, \lambda_{1}\right) \\
			&+\sum_{k<k^{\prime}} \operatorname{pen}\left(\sqrt{\left\|\beta_{k}-\beta_{k^{\prime}}\right\|_{2}^{2}+\left\|\alpha_{k}-\alpha_{k^{\prime}}\right\|_{2}^{2}+\left\|\gamma_{k}-\gamma_{k^{\prime}}\right\|_{2}^{2}}, \lambda_{2}\right) \\
			&+\sum_{k<k^{\prime}} \operatorname{pen}\left(\sqrt{\left\|\beta_{k}-\beta_{k^{\prime}}\right\|_{2}^{2}+\left\|\alpha_{k}-\alpha_{k^{\prime}}\right\|_{2}^{2}}, \lambda_{3}\right)
		\end{aligned}
	\end{aligned}
\end{equation}

We intend to minimize object function \ref{eq:obj1}. For penalty function, MCP is adopted. 
\begin{equation}
	\label{eq:estimator1}
	(\hat\beta,\hat\alpha, \hat\gamma) = \mathop{\arg\min}\limits_{(\beta, \alpha, \gamma)} \mathcal{L}(\beta, \alpha, \gamma, \pi \mid X, y)
\end{equation}


\subsubsection{Penalty Function}
\label{subsubsec:penalty-function}

TheÂ minimax concave penalty (MCP) is a penalty function to get less biased regression coefficients in sparse models. 
\begin{equation}
	\label{eq:mcp}
	pen_{(\lambda, a)}(\beta) = \left\{
	\begin{aligned}
		&\lambda |\beta| - \frac{\beta^2}{2a}, & |\beta| \leq a\lambda \\
		&\frac{a}{2}\lambda^2, & |\beta| > a\lambda
	\end{aligned}
	\right.
\end{equation}
where the derivative of the penalty function is

\begin{equation}
	pen_{(\lambda, a)}^{\prime}(\beta)= \begin{cases}\operatorname{sgn}(\beta)\left(\lambda-\frac{|\beta|}{a}\right) & |\beta| \leq a \lambda \\ 0 & \text { otherwise }\end{cases}
\end{equation}

MCP minimizes the maximum concavity. Hyperparameter $a$ is usually taken to be greater than 1.

\subsubsection{Reparameterization}
\label{subsubsec:reparameterization}

Before minimizing the object function, We have to reparameterize the model.

According to literature \cite{L1} ``In mixture models, it will be crucial to have a good estimator of $\sigma^2$ and the role of the scaling of the variance parameter is much more important than in homogeneous regression models." estimation of $\sigma$ should also been taken. However, $\hat\sigma^2$ is influenced by MCP parameter $\lambda$. 

The estimator \ref{eq:estimator1} is not equivariant under scaling of the response. More precisely, consider the transformation \ref{eq:scale}
\begin{equation}
	\label{eq:scale}
	\tilde{y} = ay,\ \quad (\tilde{\beta}, \tilde{\alpha}, \tilde{\gamma}) = a(\beta, \alpha, \gamma), \quad a > 0
\end{equation}

which leaves model \ref{eq:model-matrix-full} invariant. A reasonable estimator based on transformed data $\tilde{y}$ should lead to estimators $(\tilde{\beta}, \tilde{\alpha}, \tilde{\gamma})$ which are related to $(\beta, \alpha, \gamma)$ through $(\tilde{\beta}, \tilde{\alpha}, \tilde{\gamma}) = a(\beta, \alpha, \gamma)$. This is not the case for the estimator in \ref{eq:estimator1}. To address this drawback, penalize $|\beta|/\sigma$ instead of $|\beta|$. The same strategy applies to other parameters. Reparameterization is to facilitate this process.

\begin{equation}
\rho = \frac{1}{\sigma},\quad \beta_{new} = \frac{\beta}{\sigma}, \quad \alpha_{new} = \frac{\alpha}{\sigma}, \quad \gamma_{new} = \frac{\gamma}{\sigma}
\end{equation}

For simplicity of notation, $\beta_{new}, \alpha_{new}, \gamma_{new}$ are still written as $\beta, \alpha, \gamma$. Keep in mind that they are already reparameterized. The object function can be written as

\begin{equation}
	\label{eq:obj2}
	\begin{aligned}
		\mathcal{L}(\rho, \beta, \alpha, \gamma, \pi \mid X, y)
 		&=-\sum_{i=1}^{n} \log \sum_{k=1}^{K} \pi_{k} \frac{\rho}{\sqrt{2\pi}} \exp\{-\frac{1}{2}\rho^2(y_i - \mu_k)^2\} \\
 		&+\sum_{k=1}^{K} \sum_{j=1}^{p} \operatorname{pen}\left(\left|\beta_{kj}\right|, \lambda_{1}\right)+\sum_{k=1}^{K} \sum_{s=1}^{q} \sum_{j=1}^{p} \operatorname{pen}\left(\left|\gamma_{ksj}\right|, \lambda_{1}\right) \\
 		&+\sum_{k<k^{\prime}} \operatorname{pen}\left(\sqrt{\left\|\beta_k-\beta_{k^\prime}\right\|_{2}^{2}+\left\|\alpha_k-\alpha_{k^\prime}\right\|_{2}^{2}+\left\|\gamma_k-\gamma_{k^\prime}\right\|_{2}^{2}}, \lambda_{2}\right) \\
 		&+\sum_{k<k^{\prime}} \operatorname{pen}\left(\sqrt{\left\|\beta_k-\beta_{k^\prime}\right\|_{2}^{2}+\left\|\alpha_k-\alpha_{k^\prime}\right\|_{2}^{2}}, \lambda_{3}\right)
	\end{aligned}
\end{equation}

$\Rightarrow$
%By the routine of ADMM, it can be transformed first into a constrained optimization problem \ref{eq:opt-constrain}.

\begin{equation}
	\label{eq:opt-constrain}
	\begin{aligned}
	\operatorname{min}\ \mathcal{L}(\rho, \beta, \alpha, \gamma, \pi , u, v, w\mid X, y)
	&=-\sum_{i=1}^{n} \log \sum_{k=1}^{K} \pi_{k} \frac{\rho}{\sqrt{2\pi}} \exp\{-\frac{1}{2}\rho^2(y_i - \mu_k)^2\} \\
	&+\sum_{k=1}^{K} \sum_{j=1}^{p} \operatorname{pen}\left(\left|\beta_{kj}\right|, \lambda_{1}\right)+\sum_{k=1}^{K} \sum_{s=1}^{q} \sum_{j=1}^{p} \operatorname{pen}\left(\left|\gamma_{ksj}\right|, \lambda_{1}\right) \\
	&+\sum_{k<k^{\prime}} \operatorname{pen}\left(\sqrt{\left\|u_{k{k^\prime}}\right\|_{2}^{2}+\left\|v_{k{k^\prime}}\right\|_{2}^{2}+\left\|w_{k{k^\prime}}\right\|_{2}^{2}}, \lambda_{2}\right) \\
	&+\sum_{k<k^{\prime}} \operatorname{pen}\left(\sqrt{\left\|u_{k{k^\prime}}\right\|_{2}^{2}+\left\|v_{k{k^\prime}}\right\|_{2}^{2}}, \lambda_{3}\right) \\
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		s.t. \quad & u_{kk} = \beta_k - \beta_{k^\prime} \\
		& v_{k{k^\prime}} = \alpha_k -\alpha_{k^\prime} \\
		& w_{k{k^\prime}} = \gamma_k - \gamma_{k^\prime} \\
	\end{aligned}
	\nonumber
\end{equation}
$\Rightarrow$
%By the routine of ADMM, it can be transformed first into a constrained optimization problem \ref{eq:opt-constrain}.

\begin{equation}
	\label{eq:opt-unconstrain}
	\begin{aligned}
		\operatorname{min}\ &\mathcal{L}(\rho, \beta, \alpha, \gamma, \pi, u, v, w, \xi, \zeta, \eta \mid X, y) \\
		&=-\sum_{i=1}^{n} \log \sum_{k=1}^{K} \pi_{k} \frac{\rho}{\sqrt{2\pi}} \exp\{-\frac{1}{2}\rho^2(y_i - \mu_k)^2\} \\
		&+\sum_{k=1}^{K} \sum_{j=1}^{p} \operatorname{pen}\left(\left|\beta_{kj}\right|, \lambda_{1}\right)+\sum_{k=1}^{K} \sum_{s=1}^{q} \sum_{j=1}^{p} \operatorname{pen}\left(\left|\gamma_{ksj}\right|, \lambda_{1}\right) \\
		&+\sum_{k<k^{\prime}} \operatorname{pen}\left(\sqrt{\left\|u_{k{k^\prime}}\right\|_{2}^{2}+\left\|v_{k{k^\prime}}\right\|_{2}^{2}+\left\|w_{k{k^\prime}}\right\|_{2}^{2}}, \lambda_{2}\right) \\
		&+\sum_{k<k^{\prime}} \operatorname{pen}\left(\sqrt{\left\|u_{k{k^\prime}}\right\|_{2}^{2}+\left\|v_{k{k^\prime}}\right\|_{2}^{2}}, \lambda_{3}\right) \\ 
		&+\sum_{k<k^{\prime}}\xi_{k{k^\prime}}^T(\beta_k - \beta_{k^\prime} - u_{k{k^\prime}}) + \frac{\tau}{2}\sum_{k<k^{\prime}}\Vert\beta_k - \beta_{k^\prime} - u_{k{k^\prime}}\Vert_2^2 \\
		&+\sum_{k<k^{\prime}}\zeta_{k{k^\prime}}^T(\alpha_k - \alpha_{k^\prime} - u_{k{k^\prime}}) + \frac{\tau}{2}\sum_{k<k^{\prime}}\Vert\alpha_k - \alpha_{k^\prime} - v_{k{k^\prime}}\Vert_2^2 \\
		&+\sum_{k<k^{\prime}}\eta_{k{k^\prime}}^T(\gamma_k - \gamma_{k^\prime} - w_{k{k^\prime}}) + \frac{\tau}{2}\sum_{k<k^{\prime}}\Vert\gamma_k - \gamma_{k^\prime} - w_{k{k^\prime}}\Vert_2^2 
	\end{aligned}
\end{equation}


\section{Computation} % =====================================================
\label{sec:computation}

\subsection{Preparation}

The overall strategy is EM+ADMM (similar to algorithm of literature \cite{ggm}). Before the main computation body of the algorithm, steps of transformation are needed to facilitate the computation.

\subsubsection{Hidden Indicator $C_i$}
\label{subsubsec:hidden-indicator}

Assume there's an unobserved label $C_i = 1,...,K$ for each sample representing the sample's class. According to Bayes' Rule, posterior probability of $C_i$ is

\begin{equation}
\label{eq:qc}
\begin{aligned}
q_{C_i}(c_i) = p_{C_i|Y_i}(c_i|y_i) &= \frac{p_{Y_i|C_i}(y_i|c_i)p_{C_i}(c_i)}{p_{Y_i}(y_i)} \\
&= \frac{\displaystyle\prod_{k=1}^K\left[\pi_k f(y_i;\mu_k, \rho^2)\right]^{I(c_i=k)}}{\displaystyle\sum_{k^\prime=1}^{K}\pi_{k^\prime} f(y_i;\mu_{k^\prime}, \rho^2)} \\ 
\end{aligned}
\end{equation}

Specifically, 
\begin{equation}
q_{C_i}(k) = \frac{\pi_k f(y_i;\mu_k, \rho^2)}{\displaystyle\sum_{k^\prime=1}^{K}\pi_{k^\prime} f(y_i;\mu_{k^\prime}, \rho^2)}
\end{equation}

For log-likelihood part of the object function \ref{eq:opt-unconstrain}, we have
\begin{equation}
\label{eq:ob-loglikelihood}
\begin{aligned}
\log{p_Y(y|\beta, \alpha, \gamma)} 
&= \log{\sum_{k = 1}^{K} p_{Y, C}(y, k; \beta, \alpha, \gamma)} \\
&= \log{\sum_{k = 1}^{K}\frac{ p_{Y, C}(y, k; \beta, \alpha, \gamma)}{q_C(k)}q_C(k)} \\
&= \log{E_{q_C}\left(\frac{ p_{Y, C}(y, c; \beta, \alpha, \gamma)}{q_C(c)}\right)} \\
&\geq E_{q_C}\left[\log{\left(\frac{ p_{Y, C}(y, c; \beta, \alpha, \gamma)}{q_C(c)}\right)}\right]
\end{aligned}
\end{equation}

$Y = (y_1,...,y_n)^T, C = (C_1,...,C_n)^T$. The last inequality comes from Jensen's inequality. $\log{P_Y(y|\beta, \alpha, \gamma)} $ is hard to be maximized directly, the lower bound is considered intead. 

\begin{equation}
\label{eq:lower-bound-1}
\begin{aligned}
E_{q_C}\left[\log{\left(\frac{ p_{Y, C}(y, k; \beta, \alpha, \gamma)}{q_C(c)}\right)}\right]
&= E_{q_C}\left[\log{p_{Y, C}(y, c; \beta, \alpha, \gamma)} - \log{q_C(c)}\right] \\
&= E_{q_C}\left[\log{p_{Y, C}(y, c; \beta, \alpha, \gamma)}\right] - E_{q_C}\left[\log{q_C(c)}\right] \\
\end{aligned}
\end{equation}

The second part of equivalent form \ref{eq:lower-bound-1} is independent of $\beta, \alpha, \gamma$. Thus the M step of EM algorithm can be summarized as $(\hat\beta, \hat\alpha, \hat\gamma) = \mathop{\arg\max}\limits_{(\beta, \alpha, \gamma)}{E_{q_C}\left[\log{p_{Y, C}(y, c; \beta, \alpha, \gamma)}\right]}$

\begin{equation}
	\label{eq:lower-bound-2}
	\begin{aligned}
		E_{q_C}\left[\log{\left(\frac{p_{Y, C}(y, c; \beta, \alpha, \gamma)}{q_C(c)}\right)}\right]
		&=\log{\left(p_{Y}(y; \beta, \alpha, \gamma)\right)} - E_{q_C}\left[\log\left(\frac{q_C(c)}{p_{C|Y}(c|y)}\right)\right] \\
	\end{aligned}
\end{equation}

The first term of equivalent form \ref{eq:lower-bound-2} has nothing to do with $C$. For the E step of EM algorithm, we intend to minimize $E_{q_C}\left[\log\left(\frac{q_C(c)}{p_{C|Y}(c|y)}\right)\right]$ which is actually KL divergence $D\left(P_C(.)\Vert p_{C|Y}(.|y)\right)$. So $p_C(c) \leftarrow p_{C|Y}(c|y)$.

\subsubsection{Notation}

Integrate the original object function \ref{eq:opt-unconstrain} and conclusion of subsection \ref{subsubsec:hidden-indicator}, the object to be minimized has been transformed into function \ref{eq:object-lower}.
	
\begin{equation}
\label{eq:object-lower}
\begin{aligned}
	\operatorname{min}\ &\mathcal{L}(\rho, \beta, \alpha, \gamma, \pi, u, v, w, \xi, \zeta, \eta \mid X, y) \\
	=&-E_{q_C}\left[\log{\left(\frac{p_{Y, C}(y, c; \beta, \alpha, \gamma)}{p_C(c)}\right)}\right] \\
	&+\sum_{k=1}^{K} \sum_{j=1}^{p} \operatorname{pen}\left(\left|\beta_{kj}\right|, \lambda_{1}\right)+\sum_{k=1}^{K} \sum_{s=1}^{q} \sum_{j=1}^{p} \operatorname{pen}\left(\left|\gamma_{ksj}\right|, \lambda_{1}\right) \\
	&+\sum_{k<k^{\prime}} \operatorname{pen}\left(\sqrt{\left\|u_{k{k^\prime}}\right\|_{2}^{2}+\left\|v_{k{k^\prime}}\right\|_{2}^{2}+\left\|w_{k{k^\prime}}\right\|_{2}^{2}}, \lambda_{2}\right) \\
	&+\sum_{k<k^{\prime}} \operatorname{pen}\left(\sqrt{\left\|u_{k{k^\prime}}\right\|_{2}^{2}+\left\|v_{k{k^\prime}}\right\|_{2}^{2}}, \lambda_{3}\right) \\ 
	&+\frac{\tau}{2}\Vert H_1\beta - u + \frac{1}{\tau}\xi\Vert_2^2  \\
	&+\frac{\tau}{2}\Vert H_2\alpha - v + \frac{1}{\tau}\zeta\Vert_2^2  \\
	&+\frac{\tau}{2}\Vert H_3\gamma - w + \frac{1}{\tau}\eta\Vert_2^2 
\end{aligned}
\end{equation}

where $H_1 = \varepsilon \otimes I_p, H_2 = \varepsilon \otimes I_q, H_3 = \varepsilon \otimes I_p, \varepsilon = \{(e_k - e_{k^\prime}), k < {k^\prime}\}^T$. $e_j$ is the vector whose $j$th element is 1 and the remaining ones are 0. Specifically, $dim(H_1) = \frac{K(K-1)}{2}p\times Kp, dim(H_2) = \frac{K(K-1)}{2}q\times Kq, dim(H_3) = \frac{K(K-1)}{2}pq\times Kpq$. 

\subsection{Initialization}
\label{subsec:initialization}

There are several methods to initialize $\rho_0, \beta_0, \alpha_1, \gamma_0$, including K-means or other general clustering algorithms (applied in literature \cite{ggm}) and FMR-based method (applied in literature \cite{main}). 

Key elements of FMR are: MLE, component-wise tuning parameters (maximizing BIC to select $\lambda$) and varaible selection.

\subsection{Iteration-E step}
\label{subsec:iteration-e-step}

For $t^{th}$ iteration, we update $\pi$ as 

\begin{equation}
\label{eq:step-e1}
\pi_k^{(t+1)} = \frac{1}{n}\displaystyle\sum_{i=1}^{n}q_{C_i}^{(t+1)}(k)
\end{equation}
where
\begin{equation}
\label{eq:step-e2}
q_{C_i}^{(t+1)}(k) = \frac{\pi_k^{(t)}f_k(y_i;\mu_k)}{\displaystyle\sum_{k^\prime=1}^{K}\pi_{k^\prime}^{(t)}f_{k^\prime}(y_i;\mu_{k^\prime})}
\end{equation}

\subsection{Iteration-M step}

Overall updates in the M step of the $(t+1)$th iteration are

\begin{equation}
	\label{eq:update-m-1}
	(\beta^{(t+1)},\alpha^{(t+1)}, \gamma^{(t+1)}) = \mathop{\arg\min}\limits_{(\beta, \alpha, \gamma)} \mathcal{L}(\rho^{(t)}, \beta, \alpha, \gamma, u^{(t)}, v^{(t)}, w^{(t)}, \xi^{(t)}, \zeta^{(t)}, \eta^{(t)})
\end{equation}
\begin{equation}
	\label{eq:update-m-2}
	(u^{(t+1)},v^{(t+1)}, w^{(t+1)}) = \mathop{\arg\min}\limits_{(u,v,w)} \mathcal{L}(\rho^{(t)}, \beta^{(t+1)}, \alpha^{(t+1)}, \gamma^{(t+1)}, u, v, w, \xi^{(t)}, \zeta^{(t)}, \eta^{(t)})
\end{equation}
\begin{equation}
	\label{eq:update-m-rho}
	\rho^{(t+1)} = \mathop{\arg\min}\limits_{\rho} \mathcal{L}(\rho, \beta^{(t+1)}, \alpha^{(t+1)}, \gamma^{(t+1)}, u^{(t+1)}, v^{(t+1)}, w^{(t+1)}, \xi^{(t)}, \zeta^{(t)}, \eta^{(t)})
\end{equation}
\begin{equation}
	\label{eq:update-m-3}
	\xi_{k{k^\prime}}^{(t+1)}=\xi_{k{k^\prime}}^{(t)}+\tau\left(\beta_{k}^{(t+1)}-\beta_{k^\prime}^{(t+1)}-u_{k{k^\prime}}^{(t+1)}\right)
\end{equation}
\begin{equation}
	\label{eq:update-m-4}
	\zeta_{k{k^\prime}}^{(t+1)}=\zeta_{k{k^\prime}}^{(t)}+\tau\left(\alpha_{k}^{(t+1)}-\alpha_{k^\prime}^{(t+1)}-v_{k{k^\prime}}^{(t+1)}\right)
\end{equation}
\begin{equation}
	\label{eq:update-m-5}
	\eta_{k{k^\prime}}^{(t+1)}=\eta_{k{k^\prime}}^{(t)}+\tau\left(\gamma_{k}^{(t+1)}-\gamma_{k^\prime}^{(t+1)}-w_{k{k^\prime}}^{(t+1)}\right)
\end{equation}

\subsubsection{Update $\beta, \alpha, \gamma$} 
The purpose of this part is to solve \ref{eq:update-m-1}
\paragraph{Update $\alpha_k$}  
Terms of $\mathcal{L}$ \ref{eq:object-lower} related to $\alpha_k$ are as below: 
\begin{equation}
\mathcal{L}(\alpha_k) = -l_1 + \frac{\tau}{2}\Vert H_2\alpha - v + \frac{1}{\tau}\zeta\Vert_2^2
\end{equation}
By the chain rule, we get the partial derivative for the first term as:
\begin{equation}
\label{update-alpha-1}
\begin{aligned}
-\frac{\partial l_1}{\partial \alpha_k} &= -\frac{\partial l_1}{\partial \mu_k} \frac{\partial \mu_k}{\partial \alpha_k} \\
&= -\left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)\rho^2(y_i-\mu_k)\right)\left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)z_i\right)
\end{aligned}
\end{equation}
As for the second term,
\begin{equation}
\label{update-alpha-2}
\begin{aligned}
l_2 &= \frac{\tau}{2}\Vert H_2\alpha - v + \frac{1}{\tau}\zeta\Vert \\
&= \frac{\tau}{2} \left(\alpha^TH_2^TH_2\alpha+(v-\frac{1}{\tau}\zeta)^T(v-\frac{1}{\tau}\zeta) - 2\alpha^TH_2^T(v-\frac{1}{\tau}\zeta)\right)
\end{aligned}
\end{equation}
Take the derivative, we have
\begin{equation}
\label{update-alpha-3}
\frac{\partial l_2}{\partial \alpha} = \tau \left(H_2^T H_2\alpha - H_2^T(v-\frac{1}{\tau}\zeta)\right) 
\end{equation}
Name $t^{(\alpha)}$ as $H_2^T H_2\alpha - H_2^T(v-\frac{1}{\tau}\zeta)$, where $t^{(\alpha)}\in\mathbb{R}^{kq\times 1}$. For each $\alpha_k$, take corresponding terms as:
\begin{equation}
\label{update-alpha-4}
\frac{\partial l_2}{\partial \alpha_k} = \tau t^{(\alpha)}_{[(k-1)q:kq-1]}
\end{equation}
Specifically,
\begin{equation}
\label{eq:update_alpha-t}
t^{(\alpha)}_{[(k-1)q:kq-1]} = (H_2^T H_2)_{[(k-1)q:kq-1,:]}\alpha - H_{2{[(k-1)q:kq-1,:]}}^T (v-\frac{1}{\tau}\zeta)
\end{equation}
Thus,
\begin{equation}
	\label{update-alpha-5}
	\begin{aligned}
	\frac{\partial \mathcal{L}(\alpha_k)}{\partial \alpha_k} &= \tau t^{(\alpha)}_{[(k-1)q,kq-1]} - \left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)\rho^2(y_i-\mu_k)\right)\left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)z_i\right) \\
	\end{aligned}
\end{equation}
Let $\frac{\partial \mathcal{L}(\alpha_k)}{\partial \alpha_k} = 0$, we have 
\begin{equation}
\label{eq:update-alpha-solve}
(H_2^T H_2)_{[(k-1)q:kq-1,:]}\alpha = H_{2{[(k-1)q:kq-1,:]}}^T (v-\frac{1}{\tau}\zeta) + \frac{1}{\tau}\left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)\rho^2(y_i-\mu_k)\right)\left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)z_i\right)
\end{equation}
To make it clear, note
\begin{equation}
\label{eq:update-alpha-note}
\left\{
\begin{aligned}
&A_k:= (H_2^T H_2)_{[(k-1)q:kq-1,:]}\in \mathbb{R}^{q\times kq} \\
&b_k:=H_{2{[(k-1)q:kq-1,:]}}^T (v-\frac{1}{\tau}\zeta) + \frac{1}{\tau}\left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)\rho^2(y_i-\mu_k)\right)\left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)z_i\right) \in\mathbb{R}^{q\times 1}
\end{aligned}
\right.
\end{equation}
\ref{eq:update-alpha-solve} is actually a system of undetermined linear equations written as $A_k\alpha = b_k$ with $kq$ varaibles and $q$ equations. Pay attention that although the solutions to it is of $kq\times 1$ dim, only the slice $[(k-1)q:kq-1]$ is valid for $\alpha_k$.

Pay attention: Updates for $\alpha$ is always the same whatever the cases. It means that $\alpha$ is not necessarily sparse for no penalty is set specifically for it, which is not the case for $\beta$ and $\gamma$.

\paragraph{Update $\beta_k$}
Terms of $\mathcal{L}$ \ref{eq:object-lower} related to $\beta_k$ are as below: 
\begin{equation}
	\mathcal{L}(\beta_k) = -l_1 + \displaystyle\sum_{k=1}^{K}\sum_{j=1}^{p}pen(|\beta_{kj}|, \lambda_1) + \frac{\tau}{2}\Vert H_1\beta - u + \frac{1}{\tau}\xi\Vert_2^2
\end{equation}
Similar to $\alpha_k$ update, the chain rule is applied to $-l_1$
\begin{equation}
\label{eq:update-beta-l1}
\begin{aligned}
	-\frac{\partial l_1}{\partial \alpha_k} &= -\frac{\partial l_1}{\partial \mu_k} \frac{\partial \mu_k}{\partial \alpha_k} \\
	&= - \left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)\rho^2(y_i-\mu_k)\right)\left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)(x_i + \sum_{s=1}^{q} {W_i^{(s)}}\odot\gamma_{is})\right)
\end{aligned}
\end{equation}
For the second term,
\begin{equation}
\begin{aligned}
l_2 &= \displaystyle\sum_{k=1}^{K}\sum_{j=1}^{p}pen(|\beta_{kj}|, \lambda_1) + \frac{\tau}{2}\Vert H_1\beta - u + \frac{1}{\tau}\xi\Vert_2^2 \\
&= \displaystyle\sum_{k=1}^{K}\sum_{j=1}^{p}pen(|\beta_{kj}|, \lambda_1) + \frac{\tau}{2} \left(\beta^TH_1^TH_1\beta+(u-\frac{1}{\tau}\xi)^T(u-\frac{1}{\tau}\xi) - 2\beta^TH_1^T(u-\frac{1}{\tau}\xi)\right)
\end{aligned}
\end{equation}
For simplicity, note $t^{(\beta)} = H_1^T H_1\beta - H_1(u-\frac{1}{\tau}\xi)\in \mathbb{R}^{kp\times 1}$, then $\frac{\partial }{\partial \beta} \frac{\tau}{2}\Vert H_1\beta - u + \frac{1}{\tau}\xi\Vert_2^2= \tau t^{(\beta)}$
$\beta_{kj}$s are penalized seperately, which cannot be updated all at once. 

Now consider a specific $\beta_{kj}$
\begin{equation}
\label{eq:update-beta-l2}
\frac{\partial l_2(\beta_{kj})}{\partial \beta_{kj}} = \left\{
\begin{aligned}
	& sgn(\beta_{kj})\left(\lambda_1 - \frac{1}{a}|\beta_{kj}|\right) + \tau t^{(\beta)}_{kj}, &|\beta_{kj}| \leq a\lambda_1 \\
	& \tau t^{(\beta)}_{kj}, &|\beta_{kj}| > a\lambda_1
\end{aligned}
\right.
\end{equation}
Integrate \ref{eq:update-beta-l1} and \ref{eq:update-beta-l2}, we have
\begin{equation}
\label{eq:update-beta-l}
\frac{\partial \mathcal{L}(\beta_k)}{\partial \beta_{kj}} = \left\{
\begin{aligned}
	& \lambda_1 - \frac{1}{a}|\beta_{kj}| + \tau t^{(\beta)}_{kj} - \left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)\rho^2(y_i-\mu_k)\right)\left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)(x_i + W_i^T\gamma_i\otimes I_q)\right)_j, &|\beta_{kj}| \leq a\lambda_1 \\
	& \tau t^{(\beta)}_{kj} - \left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)\rho^2(y_i-\mu_k)\right)\left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)(x_i + W_i^T\gamma_i\otimes I_q)\right)_j, &|\beta_{kj}| > a\lambda_1
\end{aligned}
\right.
\end{equation}
which is also a system of linear equations whose form depends on $|\beta_{kj}|$.

\paragraph{Update $\gamma_k$}

Terms of $\mathcal{L}$ \ref{eq:object-lower} related to $\gamma_k$ are as below: 
\begin{equation}
	\mathcal{L}(\gamma_k) = -l_1 + \displaystyle\sum_{k=1}^{K}\sum_{j=1}^{p}\sum_{s=1}^{q}pen(|\gamma_{ksj}|, \lambda_1) + \frac{\tau}{2}\Vert H_3\gamma - w + \frac{1}{\tau}\eta\Vert_2^2
\end{equation}

Update of $\gamma_k$ is very similar to the process for $\beta_k$. Steps in details are omitted here. The partial derivative for $\gamma_{ksj}$ from $\mathcal{L}(\gamma_k)$ is as below.

\begin{equation}
\label{eq:update-gamma-l}
\frac{\partial \mathcal{L}(\gamma_k)}{\partial \gamma_{ksj}} =
\left\{
\begin{aligned}
	& \lambda_2 - \frac{1}{a}|\gamma_{ksj}| + \tau t^{(\gamma)}_{ksj} - \left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)\rho^2(y_i-\mu_k)\right)\left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)(W_i\odot(\beta_i\otimes I_q))\right)_{sj}, &|\gamma_{ksj}| \leq a\lambda_1 \\
	& \tau t^{(\gamma)}_{ksj} - \left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)\rho^2(y_i-\mu_k)\right)\left(\displaystyle\sum_{i=1}^{n}q_{C_i}(k)(W_i\odot(\beta_i\otimes I_q))\right)_{sj}, &|\gamma_{ksj}| > a\lambda_1
\end{aligned}
\right.
\end{equation}
where $t^{(\gamma)} = H_3^T H_3\gamma - H_3(w-\frac{1}{\tau}\eta)\in \mathbb{R}^{kpq\times 1}$

\subsubsection{Update $u,v,w$}
The purpose of this part is to solve \ref{eq:update-m-2}. Terms of $\mathcal{L}$ \ref{eq:object-lower} related to $u, v, w$ are as below: 
\begin{equation}
\begin{aligned}
\mathcal{L}(u,v,w) =& \sum_{k<{k^\prime}}pen(\sqrt{\Vert u_{k{k^\prime}}\Vert_2^2 + \Vert v_{k{k^\prime}}\Vert_2^2 + \Vert w_{k{k^\prime}}\Vert_2^2}, \lambda_2) \\
&+ \sum_{k<{k^\prime}}pen(\sqrt{\Vert u_{k{k^\prime}}\Vert_2^2 + \Vert v_{k{k^\prime}}\Vert_2^2}, \lambda_3) \\
&+ \frac{\tau}{2}\Vert H_1\gamma - u + \frac{1}{\tau}\xi\Vert_2^2 
+ \frac{\tau}{2}\Vert H_2\gamma - v + \frac{1}{\tau}\zeta\Vert_2^2 
+ \frac{\tau}{2}\Vert H_3\gamma - w + \frac{1}{\tau}\eta\Vert_2^2 
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\mathcal{L}(u,v,w)=& \sum_{k<{k^\prime}}pen(\sqrt{\Vert u_{k{k^\prime}}\Vert_2^2 + \Vert v_{k{k^\prime}}\Vert_2^2 + \Vert w_{k{k^\prime}}\Vert_2^2}, \lambda_2) \\
&+ \sum_{k<{k^\prime}}pen(\sqrt{\Vert u_{k{k^\prime}}\Vert_2^2 + \Vert v_{k{k^\prime}}\Vert_2^2}, \lambda_3) \\
&+ \frac{\tau}{2}\left\|\left(\begin{array}{ccc}
	H_{1} & & \\
	& H_{2} & \\
	& & H_{3}
\end{array}\right)\left(\begin{array}{l}
	\beta \\
	\alpha \\
	\gamma
\end{array}\right)-\left(\begin{array}{l}
	u \\
	v \\
	w
\end{array}\right)+\frac{1}{\tau}\left(\begin{array}{l}
	\xi \\
	\zeta \\
	\eta
\end{array}\right)\right\|_{2}^{2}
\end{aligned}
\end{equation}
Now consider one pair of $(k, k^\prime)$. Some notations are defined before put it further.
\begin{equation}
\label{eq:update-uvw-ori}
\left\{
\begin{aligned}
&\phi_{k{k^\prime}} = (u^T_{k{k^\prime}}, v^T_{k{k^\prime}}, w^T_{k{k^\prime}})^T \\
&\psi_{k{k^\prime}} = (u^T_{k{k^\prime}}, v^T_{k{k^\prime}})^T
\end{aligned}
\right.
\end{equation}
Let
\begin{equation}
	\left\{
	\begin{aligned}
		&\overline{u_{k{k^\prime}}} = \beta_k - \beta_{k^\prime} + \frac{1}{\tau}\xi_{k{k^\prime}}\\
		&\overline{v_{k{k^\prime}}} = \alpha_k - \alpha_{k^\prime} + \frac{1}{\tau}\zeta_{k{k^\prime}}\\
		&\overline{w_{k{k^\prime}}} = \gamma_k - \gamma_{k^\prime} + \frac{1}{\tau}\eta_{k{k^\prime}}
	\end{aligned}
	\right.
\end{equation}
Then, 
\begin{equation}
	\left\{
	\begin{aligned}
		&\overline{\phi_{k{k^\prime}}} = (\overline{u_{k{k^\prime}}}^T, \overline{v_{k{k^\prime}}}^T, \overline{w_{k{k^\prime}}}^T)^T \\
		&\overline{\psi_{k{k^\prime}}} = (\overline{u_{k{k^\prime}}}^T, \overline{v_{k{k^\prime}}}^T)^T
	\end{aligned}
	\right.
\end{equation}

\ref{eq:update-uvw-ori} is rewritten as 

\begin{equation}
	\label{eq:update-uvw-formal}
	\begin{aligned}
		\mathcal{L}(u,v,w)=& \sum_{k<{k^\prime}}pen(\sqrt{\Vert \phi_{k{k^\prime}}\Vert_2^2}, \lambda_2) 
		+\sum_{k<{k^\prime}}pen(\sqrt{\Vert \psi_{k{k^\prime}}\Vert_2^2}, \lambda_3) \\
		&+ \frac{\tau}{2}\left\|\left(\begin{array}{l}
			u_{k k^{\prime}} \\
			v_{k k^{\prime}} \\
			w_{k k^{\prime}}
		\end{array}\right)-\left(\begin{array}{c}
			\overline{u_{k k^{\prime}}} \\
			\overline{v_{k k^{\prime}}} \\
			\overline{w_{k k^{\prime}}}
		\end{array}\right)\right\|_{2}^{2}
	\end{aligned}
\end{equation}
\paragraph{Case 1} $\Vert\phi_{k{k^\prime}}\Vert_2 > a\lambda_2$ and $\Vert\psi_{k{k^\prime}}\Vert_2 > a\lambda_3$
\begin{equation}
\label{eq:update_uvw-case1}
\mathcal{L}(u,v,w) = \frac{a}{2}\lambda_2^2 + \frac{a}{2}\lambda_3^2 + \frac{\tau}{2}\Vert\phi_{k{k^\prime}}-\overline\phi_{k{k^\prime}}\Vert_2^2
\end{equation}
Take the partial derivatives
\begin{equation}
\label{eq:update_uvw-case1-p}
\frac{\partial \mathcal{L}(u,v,w)}{\partial \phi_{k{k^\prime}}} = \tau (\phi_{k{k^\prime}} - \overline\phi_{k{k^\prime}})
\end{equation}
Let \ref{eq:update_uvw-case1-p} equals to 0, we have $\phi_{k{k^\prime}}^{(t+1)} = \overline\phi_{k{k^\prime}}$.

When $\Vert\overline\phi_{k{k^\prime}}\Vert_2 > a\lambda_2$ and $\Vert\overline\psi_{k{k^\prime}}\Vert_2 > a\lambda_3$, $\phi_{k{k^\prime}}^{(t+1)} = \overline\phi_{k{k^\prime}}$.

\paragraph{Case 2} $\Vert\phi_{k{k^\prime}}\Vert_2 \leq a\lambda_2$ and $\Vert\psi_{k{k^\prime}}\Vert_2 > a\lambda_3$
\begin{equation}
	\label{eq:update_uvw-case2}
	\mathcal{L}(u,v,w) =  \frac{\tau}{2}\Vert\phi_{k{k^\prime}}-\overline\phi_{k{k^\prime}}\Vert_2^2 + \lambda_2 \Vert\phi_{k{k^\prime}}\Vert_2 - \frac{1}{2a}\Vert\phi_{k{k^\prime}}\Vert_2^2 + \frac{a}{2}\lambda_3^2
\end{equation}
\begin{itemize}
	\item if $\Vert\overline\phi_{k{k^\prime}}\Vert_2 = 0$, then $\phi^{(t+1)}_{k{k^\prime}} = 0$;
	\item if $\Vert\overline\phi_{k{k^\prime}}\Vert_2 \neq 0$
	\begin{itemize}
		\item When $\Vert\phi_{k{k^\prime}}\Vert_2 = 0$, $\mathcal{L}(u,v,w) = \frac{\tau}{2}\Vert\overline\phi_{k{k^\prime}}\Vert_2^2 + \frac{a}{2}\lambda_3^2$;
		\item When $\Vert\phi_{k{k^\prime}}\Vert_2 \neq 0$
	\begin{equation}
	\frac{\partial \mathcal{L}(u,v,w)}{\partial \phi_{k{k^\prime}}} = \tau (\phi_{k{k^\prime}} - \overline\phi_{k{k^\prime}}) + \left(\frac{\lambda_2}{\Vert\phi_{k{k^\prime}}\Vert_2} - \frac{1}{a}\right)\phi_{k{k^\prime}}
	\end{equation}
	Let $\frac{\partial \mathcal{L}(u,v,w)}{\partial \phi_{k{k^\prime}}} = 0$,
	\begin{equation}
	\label{eq:update-uvw-case2-a1}
	\phi_{k{k^\prime}}\left(\tau + \frac{\lambda_2}{\Vert\overline\phi_{k{k^\prime}}\Vert_2} - \frac{1}{a}\right) = \tau \overline\phi_{k{k^\prime}}
	\end{equation}
	Take transposition of \ref{eq:update-uvw-case2-a1}
	\begin{equation}
	\label{eq:update-uvw-case2-a11}
	\phi_{k{k^\prime}} = \frac{\overline\phi_{k{k^\prime}}\Vert\phi_{k{k^\prime}}\Vert_2}{(1-\frac{1}{a\tau})\Vert\phi_{k{k^\prime}}\Vert_2 + \frac{\lambda_2}{\tau}}
	\end{equation}
	Take the square root of both sides of equation \ref{eq:update-uvw-case2-a1}
	\begin{equation}
	\label{eq:update-uvw-case2-a12}
	\Vert\phi_{k{k^\prime}}\Vert_2 = \frac{\Vert\overline\phi_{k{k^\prime}}\Vert_2-\frac{\lambda_2}{\tau}}{1-\frac{1}{a\tau}}
	\end{equation}
	Plug \ref{eq:update-uvw-case2-a12} into \ref{eq:update-uvw-case2-a11}, then the update is derived.
	\begin{equation}
	\label{eq:update-uvw-case2-final}
	\phi_{k{k^\prime}}^{(t+1)} = \frac{\left(1 - \frac{\lambda_2}{\tau}\frac{1}{\Vert\overline\phi_{k{k^\prime}}\Vert_2}\right)_+}{1-\frac{1}{a\tau}} \cdot \overline\phi_{k{k^\prime}}
	\end{equation}
	if $\frac{\left(1 - \frac{\lambda_2}{\tau}\frac{1}{\Vert\overline\phi_{k{k^\prime}}\Vert_2}\right)_+}{1-\frac{1}{a\tau}} \Vert\overline\phi_{k{k^\prime}}\Vert_2 \leq a\lambda_2$ and $\frac{\left(1 - \frac{\lambda_2}{\tau}\frac{1}{\Vert\overline\phi_{k{k^\prime}}\Vert_2}\right)_+}{1-\frac{1}{a\tau}} \Vert\overline\psi_{k{k^\prime}}\Vert_2 > a\lambda_3$.
	\end{itemize}
\end{itemize}

\paragraph{Case 3} $\Vert\phi_{k{k^\prime}}\Vert_2 > a\lambda_2$ and $\Vert\psi_{k{k^\prime}}\Vert_2 \leq a\lambda_3$
\begin{equation}
	\label{eq:update_uvw-case3}
	\mathcal{L}(u,v,w) =  \frac{\tau}{2}\Vert\phi_{k{k^\prime}}-\overline\phi_{k{k^\prime}}\Vert_2^2 + \lambda_3 \Vert\psi_{k{k^\prime}}\Vert_2 - \frac{1}{2a}\Vert\psi_{k{k^\prime}}\Vert_2^2 + \frac{a}{2}\lambda_2^2
\end{equation}
Very much similar to case 2, we have update
\begin{equation}
\label{eq:update-uvw-case3-final}
\left\{
\begin{aligned}
& w^{(t+1)}_{k{k^\prime}} = \overline{w_{k{k^\prime}}} \\
& \psi_{k{k^\prime}}^{(t+1)}= \frac{\left(1 - \frac{\lambda_3}{\tau}\frac{1}{\Vert\overline\psi_{k{k^\prime}}\Vert_2}\right)_+}{1-\frac{1}{a\tau}}\cdot \overline{\psi_{k{k^\prime}}}
\end{aligned}
\right.
\end{equation}
if $\frac{\left(1 - \frac{\lambda_3}{\tau}\frac{1}{\Vert\overline\psi_{k{k^\prime}}\Vert_2}\right)_+}{1-\frac{1}{a\tau}} \Vert\overline{\psi_{k{k^\prime}}}\Vert_2 \leq a\lambda_3$ and $\left[\frac{\left(1 - \frac{\lambda_3}{\tau}\frac{1}{\Vert\overline\psi_{k{k^\prime}}\Vert_2}\right)_+}{1-\frac{1}{a\tau}}\right]^2 \Vert\overline{\psi_{k{k^\prime}}}\Vert_2^2 + \Vert\overline{w_{k{k^\prime}}}\Vert_2^2 > a^2\lambda_2^2$

\paragraph{Case 4} $\Vert\phi_{k{k^\prime}}\Vert_2 \leq a\lambda_2$ and $\Vert\psi_{k{k^\prime}}\Vert_2 \leq a\lambda_3$
\begin{equation}
	\label{eq:update_uvw-case4}
	\begin{aligned}
		\mathcal{L}(u,v,w) &=
		\frac{\tau}{2}\Vert\phi_{k{k^\prime}}-\overline\phi_{k{k^\prime}}\Vert_2^2 + pen(\Vert\phi_{k{k^\prime}}\Vert_2, \lambda_2) + pen(\Vert\psi_{k{k^\prime}}\Vert_2, \lambda_3) \\
		&= \frac{\tau}{2}\Vert\phi_{k{k^\prime}}-\overline\phi_{k{k^\prime}}\Vert_2^2 + \lambda_2 \Vert\phi_{k{k^\prime}}\Vert_2 - \frac{1}{2a}\Vert\phi_{k{k^\prime}}\Vert_2^2 + \lambda_3 \Vert\psi_{k{k^\prime}}\Vert_2 - \frac{1}{2a}\Vert\psi_{k{k^\prime}}\Vert_2^2
	\end{aligned}
\end{equation}

Take the partial derivative for $u_{k{k^\prime}}$
\begin{equation}
	\label{eq:update_uvw_der}
	\frac{\partial \mathcal{L}(u,v,w)}{\partial u_{k{k^\prime}}} =
	\tau(u_{k{k^\prime}} - \overline u_{k{k^\prime}}) + pen'(\Vert\phi_{k{k^\prime}}\Vert_2, \lambda_2)\frac{u_{k{k^\prime}}}{\Vert\phi_{k{k^\prime}}\Vert_2} + pen'(\Vert\psi_{k{k^\prime}}\Vert_2, \lambda_3)\frac{u_{k{k^\prime}}}{\Vert\psi_{k{k^\prime}}\Vert_2}
\end{equation}

The local quadratic approximation technique is applied here, which leads to an
explicit solution at each iteration. $pen'(\Vert\phi_{k{k^\prime}}\Vert_2, \lambda_2)$ is approximated by $pen'(\Vert\phi^{(t)}_{k{k^\prime}}\Vert_2, \lambda_2)$ for $\phi^{(t)}_{k{k^\prime}}$ is close to $\phi_{k{k^\prime}}$ and almost the same when the algorithm converges.

\begin{equation}
	\label{eq:local_quadratic_approximation}
	\begin{aligned}
	pen'(\Vert\phi_{k{k^\prime}}\Vert_2, \lambda_2)\frac{1}{\Vert\phi_{k{k^\prime}}\Vert_2} 
	&= pen'(\Vert\phi^{(t)}_{k{k^\prime}}\Vert_2, \lambda_2)\cdot\frac{1}{\Vert\phi^{(t)}_{k{k^\prime}}\Vert_2} \cdot\Vert\phi_{k{k^\prime}}\Vert_2 \cdot \frac{1}{\Vert\phi_{k{k^\prime}}\Vert_2} \\
	&= pen'(\Vert\phi^{(t)}_{k{k^\prime}}\Vert_2, \lambda_2)\frac{1}{\Vert\phi^{(t)}_{k{k^\prime}}\Vert_2}
	\end{aligned}
\end{equation}

Similarly, $pen'(\Vert\psi_{k{k^\prime}}\Vert_2, \lambda_3)\frac{1}{\Vert\psi_{k{k^\prime}}\Vert_2} \approx pen'(\Vert\psi^{(t)}_{k{k^\prime}}\Vert_2, \lambda_3)\frac{1}{\Vert\psi^{(t)}_{k{k^\prime}}\Vert_2}$. 

Let equation \ref{eq:update_uvw_der} equals to 0, the update for $u_{k{k^\prime}}$ is derived. Cases are the same for $v_{k{k^\prime}}$ and $w_{k{k^\prime}}$. If not the first three cases, $u^{(t+1)}_{k{k^\prime}}, v^{(t+1)}_{k{k^\prime}}, w^{(t+1)}_{k{k^\prime}}$ are as below.

\begin{equation}
\label{eq:update-uvw-case4-final}
\left\{
\begin{aligned}
& u_{k{k^\prime}}^{(t+1)} = \frac{\overline{u_{k{k^\prime}}}^{(t+1)}}{1+\frac{pen'(\Vert\phi^{(t)}_{k{k^\prime}}\Vert_2, \lambda_2)}{\tau\Vert\phi^{(t)}_{k{k^\prime}}\Vert_2} + \frac{pen'(\Vert\psi^{(t)}_{k{k^\prime}}\Vert_2, \lambda_3)}{\tau\Vert\psi^{(t)}_{k{k^\prime}}\Vert_2}} \\
& v_{k{k^\prime}}^{(t+1)} = \frac{\overline{v_{k{k^\prime}}}^{(t+1)}}{1+\frac{pen'(\Vert\phi^{(t)}_{k{k^\prime}}\Vert_2, \lambda_2)}{\tau\Vert\phi^{(t)}_{k{k^\prime}}\Vert_2} + \frac{pen'(\Vert\psi^{(t)}_{k{k^\prime}}\Vert_2, \lambda_3)}{\tau\Vert\psi^{(t)}_{k{k^\prime}}\Vert_2}} \\
& w_{k{k^\prime}}^{(t+1)} = \frac{\overline{w_{k{k^\prime}}}^{(t+1)}}{1+\frac{pen'(\Vert\phi^{(t)}_{k{k^\prime}}\Vert_2, \lambda_2)}{\tau\Vert\phi^{(t)}_{k{k^\prime}}\Vert_2}} \\
\end{aligned}
\right.
\end{equation}

where $pen'(\Vert\phi^{(t)}_{k{k^\prime}}\Vert_2, \lambda_2) = \lambda_2 - \frac{1}{a} \Vert\phi^{(t)}_{k{k^\prime}}\Vert_2$, $pen'(\Vert\psi^{(t)}_{k{k^\prime}}\Vert_2, \lambda_3) = \lambda_3 - \frac{1}{a} \Vert\psi^{(t)}_{k{k^\prime}}\Vert_2$.

\subsubsection{Update $\xi, \zeta, \eta$}
For each pair of $(k, k^\prime)$, updates go as equation \ref{eq:update-m-3}, \ref{eq:update-m-4} and \ref{eq:update-m-5}, which are copied as below.

\begin{equation}
	\xi_{k{k^\prime}}^{(t+1)}=\xi_{k{k^\prime}}^{(t)}+\tau\left(\beta_{k}^{(t+1)}-\beta_{k^\prime}^{(t+1)}-u_{k{k^\prime}}^{(t+1)}\right)
\end{equation}
\begin{equation}
	\zeta_{k{k^\prime}}^{(t+1)}=\zeta_{k{k^\prime}}^{(t)}+\tau\left(\alpha_{k}^{(t+1)}-\alpha_{k^\prime}^{(t+1)}-v_{k{k^\prime}}^{(t+1)}\right)
\end{equation}
\begin{equation}
	\eta_{k{k^\prime}}^{(t+1)}=\eta_{k{k^\prime}}^{(t)}+\tau\left(\gamma_{k}^{(t+1)}-\gamma_{k^\prime}^{(t+1)}-w_{k{k^\prime}}^{(t+1)}\right)
\end{equation}

\subsubsection{Update $\rho$}
Only log-likelihood part of $\mathcal{L}$ \ref{eq:object-lower} is related to $\rho$
\begin{equation}
\begin{aligned}
\mathcal{L}(\rho) &= E_{q_C}\left[\ln{p_{Y,C}(y,c;\beta,\alpha,\gamma)}\right] \\
&= \displaystyle\sum_{i=1}^{n}\sum_{k=1}^{K} q_{C_i}(k)\left[\ln \pi_k + \ln\frac{\rho}{\sqrt{2\pi}} - \frac{1}{2}\rho^2(y_i-\mu_k)^2\right]
\end{aligned}
\end{equation}
Take the derivative, we have
\begin{equation}
\label{eq:update-rho-p}
\frac{\partial \mathcal{L}(\rho)}{\partial \rho_{k}} = \displaystyle\sum_{i=1}^{n} q_{C_i}(k)\left[\frac{1}{\rho} - \rho(y_i - \mu_k)^2\right]
\end{equation}
Let \ref{eq:update-rho-p} equals to 0, then
\begin{equation}
\label{eq:update-rho}
(\rho^{2}_{k})^{(t+1)} = \frac{ \displaystyle\sum_{i=1}^{n} q_{C_i}(k)}{ \displaystyle\sum_{i=1}^{n} q_{C_i}(k)(y_i - \mu_k)^2}
\end{equation}

\subsection{Algorithm}
%\begin{algorithm}[H]
%	\SetAlgoLined
%	\KwData{$X, Z, y$}
%	\KwResult{$C_i,i=1,...,n, \beta_k,\alpha_k,\gamma_k,k=1,...,K$}
%	initialization\;
%	\While{not at end of this document}{
%		read current\;
%		\eIf{understand}{
%			go to next section\;
%			current section becomes this one\;
%		}{
%			go back to the beginning of current section\;
%		}
%	}
%	\caption{}
%\end{algorithm}

\section{Simulation}
\label{sec:simulation}

\subsection{Data Generation}
\label{subsec:data-generation}

\paragraph{Covariates}
For $n=120$ independent subjects, we generate $x_i$'s from normal distribution $N_p(0, \Sigma_1)$, where $\Sigma_1 = (\sigma_{1jm})$ with $\sigma_{jm}=1$ if $j=m$, and $\sigma_{jm}=0.2$ otherwise. $z_i$'s are generated from normal distribution $N_q(0, \Sigma_2)$, where $\Sigma_2 = (\sigma_{2jm})$ with $\sigma_{jm}=1$ if $j=m$, and $\sigma_{jm}=0.2$ otherwise. There is no correlation between $X$ and $Z$, that is $Cov(x_i, z_s) = 0, i=1,...,p, s=1,...,q$.

\paragraph{Coefficients}
There are $K_1 = 2$ main groups with $\beta = (\mu, \mu, 0_{p-2}) \in \mathbb{R}^{p}, \alpha =  (\mu, \mu, 0_{q-2}) \in \mathbb{R}^{q}$ and $\beta =(-\mu, -\mu, 0_{p-2}) \in \mathbb{R}^{p}, \alpha = (-\mu, -\mu, 0_{q-2}) \in \mathbb{R}^{q}$. There are $K_2 = 4$ subgroups with $\gamma$ equals to $(1.5\mu,1.5\mu,1.5\mu,1.5\mu,0_{pq-4})$, $(0.5\mu,0.5\mu,0.5\mu,0.5\mu,0_{pq-4})$, $(-0.5\mu,-0.5\mu,-0.5\mu,-0.5\mu,0_{pq-4})$,  \\$(-1.5\mu,-1.5\mu,-1.5\mu,-1.5\mu,0_{pq-4})$.

For the proportions of subjects in the sub-subgroups, we consider the balanced condition $(1/4,1/4,1/4,1/4)$.

\paragraph{Response Variable}

Since we have all covariates and true coefficients generated, $y_i$'s are derived by \ref{eq:model-matrix}. Notice that not $\gamma_i$'s but $I_{q\times 1}\otimes \beta_i\odot \gamma_i$'s are not coefficients for $W_i$.

\subsection{Estimation}
\label{subsec:estimation}
\paragraph{Subgrouping Consistency}

\begin{equation}
	\label{eq:sc}
	\begin{aligned}
		\operatorname{SC}(\widehat{\varphi}, \varphi) =\left(\begin{array}{l}
			n \\
			2
		\end{array}\right)^{-1} & \mid\left\{(i, j):  I\left(\widehat{\varphi}\left(x_{i}, z_{i}\right)=\widehat{\varphi}\left(x_{j}, z_{j}\right)\right)\right.\\
		&\left.=I\left(\varphi\left(x_{i}, z_{i}\right)=\varphi\left(x_{j}, z_{j}\right)\right) ; i<j\right\} \mid
	\end{aligned}
\end{equation}

where $\hat\phi$ and $\phi$ are the estimated and true subgrouping memberships respectively. Estimator \ref{eq:sc} is essentially the Rand index, which has been
commonly adopted to measure clustering accuracy.


\section{Test}

\subsection{Simple Set1-Linear}
\label{subsec:test-set1}

\subsubsection{Estimation}

å°è¯æç®åçè®¾å®éªè¯ä»£ç çåç¡®æ§ï¼æ¨¡åç®åä¸ºå¤åç±»çåå½é®é¢ï¼å¨ç±»å«åé¨æ

\begin{equation}
	Y_i = X_i^T \beta_k, \text{ if } i \in \text{ Class }k
\end{equation}

ç®æ å½æ°ç°ç®åä¸º minimize è´å¯¹æ°ä¼¼ç¶å½æ°

\begin{equation}
	l = -log P_Y (y)  = -\sum_{i = 1}^{n} log\sum_{k=1}^K \pi_k f_k(y_i|X_i,\beta_k)
\end{equation}

å¼å¥ hidden varaible $C_i$ ç¨ä»¥æç¤ºç¬¬ $i$ ä¸ªæ ·æ¬æå±ç±»å«ï¼å¶åéªåå¸åå¸åä¸º

\begin{equation}
	q_{C_i}(c_i) = \frac{\displaystyle\prod_{k=1}^{K}{\left(\pi_k f_k(y_i)\right)^{I(C_i=k)}}}{\displaystyle\sum_{k=1}^{K}\pi_{k^\prime} f_{k^\prime}(y_i)}
\end{equation}

è¿èï¼æ ¹æ®[Gaussian mixture models and the EM algorithm]ï¼

\begin{equation}
	\begin{aligned}
	log_Y (y)  &= log \sum_{k = 1}^{K}p_{Y,C}(y,c) \\
	&= log E_{q_C}\left(\frac{p_{Y,C}(y,c)}{q_C(c)}\right) \\
	&\geq E_{q_C}\left(\frac{p_{Y,C}(y,c)}{q_C(c)}\right) \\
	&= E_{q_C} \left(log p_{Y,C}(y,c)\right) - E_{q_C}\left(log q_C(c)\right)
	\end{aligned}
\end{equation}

å¶ä¸­ $E_{q_C}\left(log q_C(c)\right)$ ä¸ $\beta_k$ åå¼æ å³ï¼å¯¹åæ°è¿è¡ä¼°è®¡æ¶ä»èèç¬¬ä¸é¡¹ï¼æ­¤æ¶é®é¢è½¬åä¸ºæå¤§å $E_{q_C} log p_{Y,C}(y,c)$

\begin{equation}
	\begin{aligned}
		E_{q_C}log p_{Y,C}(y,k) &= E_{q_C} log \left(p_C(c) p_{Y|C}(y)\right)\\
		&= E_{q_C} log \left(\prod^{n}_{i=1} p_{C_i}(c_i) p_{Y_i|C_i}(y_i) \right) \\
		&= E_{q_C} log \left(\prod^{n}_{i=1}\prod^{K}_{k=1} \pi_{c_i} f_{Y_i|C_i}(y_i) \right)^{I(C_i=k)} \\
		&= E_{q_C} \sum^{n}_{i=1}\sum^{K}_{k=1} I(C_i=k)\left( log\pi_{c_i} + log f_{Y_i|C_i}(y_i) \right) \\
		&= \sum^{n}_{i=1}\sum^{K}_{k=1} E_{q_C} I(C_i=k)\left( log\pi_{c_i} + log f_{Y_i|C_i}(y_i) \right) \\
		&= \sum^{n}_{i=1}\sum^{K}_{k=1} q_{C_i}(k)\left( log\pi_{c_i} + log f_{Y_i|C_i}(y_i) \right) \\
		&= \sum^{n}_{i=1}\sum^{K}_{k=1} q_{C_i}(k)\left( log\pi_{c_i} + log \rho_k - \frac{1}{2} log 2\pi - \frac{\rho_k^2}{2}(y_i - \mu_{ik})^2 \right) 
	\end{aligned}
\label{eq:l}
\end{equation}

å¶ä¸­ $C$ ä¸ºä¸åæ°æ å³çé¡¹ï¼å¯¹äºç±»å« $k$ï¼ä¼åç®æ ä¸º

\begin{equation}
	\text{max}\qquad \displaystyle\sum_{i=1}^{n}q_{C_i}(k)\left(C-\frac{\rho_k^2}{2}(y_i - X_i^T \beta_k)^2 \right)
\end{equation}
$\iff$
\begin{equation}
	\begin{aligned}
		\text{min}\qquad l_k &=  \displaystyle\sum_{i=1}^{n}q_{C_i}(k)\left(y_i - X_i^T \beta_k\right)^2 \\
		&= (y - X\beta_k)^T W_k (y - X\beta_k)
	\end{aligned}
\end{equation}
å¶ä¸­ $W_k = diag(q_{C_1}(k),...,q_{C_n}(k))$, $y = (y_1,...,y_n)^T\in \mathbb{R}^{n\times 1}$, $X = (X_1, ..., X_n)^T\in \mathbb{R}^{n\times p}$, $\beta_k \in \mathbb{R}^{p\times 1}$

è½¬åä¸ºå ææå°äºä¹é®é¢ï¼æç® \cite{wls} page 3ï¼ï¼è§£ä¸º
\begin{equation}
	\hat \beta_k = (X^T W_k X)^{-1}X^T W_k y
\end{equation}

$\rho_k$ çéåä¼å¯¹åç±»ææäº§çå½±åãå¯ä»¥è®¤ä¸ºåç±»å«å±ç¨ä¸ä¸ª $\rho$ï¼å³è®¤ä¸ºåä¸ªç±»å«åé¨çéæºè¯¯å·®ç¦»æ£ç¨åº¦ç¸åï¼ï¼è¿ç§æåµä¸çæ´æ°å¼ä¸º \ref{eq:update-rho-same}ï¼è¿å¯ä»¥è®¤ä¸ºåç±»å« $\rho_k$ åå«æ´æ°ä¸å±ç¨ç¸åå¼ï¼æ­¤æ¶æ´æ°å¼ä¸º \ref{eq:update-rho-different}ï¼æ³¨æå¨è¿ç§æåµä¸å¤æ­æ ·æ¬å½ç±»ä¸ä»åå³äºå¶è·ç¦»åªä¸ªç±»å«çä¸­å¿æ´è¿ï¼è¿åç±»å« $\rho_k$ å½ååå¼çå½±åï¼å½ä¸ä¸ªç±»å«åæ°æ¶ææåµè¾å¥½æ¶ï¼å¯¹åº $\rho_k$ åºå¾å¤§ï¼$\sigma_k = 1/\rho_k$ï¼ã

å¨æµè¯å®éªä¸­ï¼ä¼åæ¶å°è¯ä¸¤ç§ç­ç¥ï¼å±ç¨ $\rho$ ä¼åè½»æ ·æ¬æ æ³å½ç±»å°è·ç¦»å¶æè¿çç±»å«ä¸­å¿çé®é¢ï¼ä½¿ç¨ä¸å $\rho_k$ å¯ä»¥æ¹ä¾¿è§æµå°åä¸ªç±»å«çæ¶ææåµã

\begin{equation}
	\hat\rho^{(0)} = n/\left(\sum_{i=1}^{n}\sum_{k=1}^{K}(y_i - \mu_{ik})^2\right)
	\label{eq:update-rho-same}
\end{equation}

\begin{equation}
	\hat\rho_k^{(0)} = \left(\sum_{i=1}^{n}q_{C_i}(k)\right)/\left(\sum_{i=1}^{n}\sum_{k=1}^{K}(y_i - \mu_{ik})^2\right), k=1,...,K
	\label{eq:update-rho-different}
\end{equation}

\subsubsection{Numerical Setting}

åä¸ªç±»å«çå®åæ°è®¾ç½®è§è¡¨ \ref{tb:coef_true_simple}

\begin{table}[h]
	\centering
	\caption{æç®æåµçåæ°è®¾å®}
	\begin{tabular}{ccccc}
		\toprule
		& Comp.1 & Comp.2 & Comp.3 & Comp.4 \\
		\midrule
		$\beta_{k1}$  & -2      & -1      & 1      & 2     \\
		$\beta_{k2}$  & -2      & -1      & 1      & 2     \\
		$\beta_{k3}$  & -2      & -1      & 1      & 2      \\
		$\beta_{k4}$  & -2      & -1      & 1      & 2      \\
		\bottomrule
	\end{tabular}
	\label{tb:coef_true_simple}
\end{table}

\subsubsection{Results}

ä½¿ç¨ \textit{flexmix} åå \textit{fmrs} åè¿è¡å¯¹æ¯ã

\textit{fmrs}ï¼éæºåå§åï¼$N(0,1)$ï¼åå¾å°æ­£ç¡®ç»æã

\textit{flexmix}ï¼æ éç»å®åå§å¼ï¼å¾å°æ­£ç¡®ç»æï¼åæ¶ï¼å¦æä¸¤ä¸ªç±»å«è·ç¦»å¾è¿ï¼ä¼èªå¨åå¹¶ä¸ºä¸ç±»ã

\ref{alg:simplest}  ä¸º EM ç®æ³è®¡ç®æµç¨ï¼ä»¥ä¸åç±»å«å±ç¨ç¸å $\rho$ ä¸ºä¾ï¼ä¸åç±»å«åå«æ´æ° $\rho_k$ çæµç¨ä¹ç±»ä¼¼ï¼åªéè¦æ¿æ¢ $\rho_k$ çæ´æ°å¬å¼å³å¯ã

\IncMargin{1em} % ä½¿å¾è¡å·ä¸åå¤çªåº 
\begin{algorithm}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output} % æ¿æ¢å³é®è¯
	
	\Input{è§æµå°ç $y$ï¼æå¤§ç±»å«ä¸ªæ° $K$ï¼åæ°ç»´åº¦ $p$}
	\Output{åæ ·æ¬çåç±»ç»æï¼$\beta_k,k=1,...,K$ çä¼°è®¡å¼}
	\BlankLine
	
	åå§å $\hat{\beta_k}^{(0)}$ï¼æ­£æåå¸éæºæ°ï¼;\\
	åå§å $\pi_k^{(0)} = 1/K, k = 1,...,K$; \\
	iter = 1.\\
	\Repeat
	{æ¶æï¼åæ°å·®çäºèæ°å°äºä¸´çå¼ï¼}
	{
		è®¡ç®åºäº $\hat{\beta_k}^{(iter-1)}$ å¾å°çåéªæ¦ç $q_C$\\
		\eIf{$\sum_{{k^\prime}=1}^{K}\pi_{k^\prime} f_{k^\prime}(y_i) \neq 0$ and $iter > 1$}{
		$q^{(iter)}_{C_i}(k) = \frac{{\pi_k^{(iter-1)} f_k(y_i;\hat{\beta_{k}}^{(iter-1)})}}{\sum_{{k^\prime}=1}^{K}\pi^{(iter-1)}_{k^\prime} f_{k^\prime}(y_i;\hat{\beta_{k\prime}}^{(iter-1)})}, k = 1,...,K$ \\
		}{
		$q^{(iter)}_{C_i}(k) = \frac{\pi^{(iter-1)}_{k}/ \Vert y_i-X_i^T\hat{\beta_k}^{(iter-1)}\Vert^2_2}{\sum_{{k^\prime}=1}^{K}\pi^{(iter-1)}_{k^\prime}/\Vert y_i-X_i^T\hat{\beta_{k^\prime}}^{(iter-1)}\Vert_2^2}, k = 1,...,K$
		}
		æ´æ° $\pi^{(iter)}_k = \frac{1}{n}\sum_{i=1}^{n}q^{(iter)}_{C_i}(k)$ \\
		\For {$k\ in\ 1:K$}{
			$W_k = diag(q_{C_1}(k),...,q_{C_n}(k))$;\\
			æ´æ° $\hat{\beta_k}^{(iter)} = (X^T W_k X)^{-1}X^T W_k y$;\\
		} 
	æ´æ° $\hat{\rho^2}^{(iter)} = n/\left({\sum_{i=1}^{n}\sum_{k=1}^{K}q^{(iter)}_{C_i}(k)(y_i - X_i^T \hat{\beta_k}^{(iter)})^2}\right)$; \\
	iter = iter + 1
	}
	\caption{æç®è®¾å®ä¸çç®æ³ï¼åç±»å«å±ç¨ $\rho$ï¼}
	\label{alg:simplest}
\end{algorithm}
\DecMargin{1em}

\begin{enumerate}
	\item æ¶æææï¼ä½¿ç¨ä¸è°ç¨ fmrs åæ¶ç¸ååå¼ç´æ¥è¿è¡è®¡ç®ï¼å³ç¸åä¸ç» $N(0, 4)$ çéæºæ°åå§åï¼è¿­ä»£çº¦13æ­¥å¾å°æ­£ç¡®ç»æï¼æ¹åæ¹å·®ç»æç±»ä¼¼ï¼è¥å¨ 0 åå§ååä¸ªç±»å«çåæ°ä¼°è®¡å§ç»ä¸è´ï¼æ æ³æ¶æã
	\item æ¶ææ¡ä»¶ï¼åæ­¢è¿­ä»£ï¼ï¼ä¸¤æ¬¡è¿­ä»£å¾å°çåæ°ä¼°è®¡å¼ $\Vert\hat{\beta_k}^{(iter)}-\hat{\beta_k}^{(iter-1)}\Vert_2^2$ å°äºä¸´çå¼ï¼æ»¡è¶³è¯¥æ¡ä»¶æªå¿æ¶æå°çå¼ï¼ä½ç»§ç»­è¿ç®ä¸ä¼åæååã\\å¨å®éè¿ç®ä¸­åç°ï¼å½åæ°ä¼°è®¡è¿ä¹æ­£ç¡®æ¶ $\hat{\rho}_k$ ä¼éå¸¸å¤§ï¼å³ $\sigma_k$ ä¼å¾å°ï¼å¯¼è´æ ·æ¬å¨ä¸åé«æ¯åå¸å¯åº¦å½æ°çåå¼å¨ä¸º 0ï¼æ­¤æ¶ä½¿ç¨çå¼è·ç¦»åç±»å«ä¸­å¿çæ¬§æ°è·ç¦»è®¡ç®æ¦çï¼ä¿è¯ä»£ç æ­£å¸¸è¿è¡çåæ¶éä½æ¬è½® $\hat\rho$ï¼ä»¥æ´å¥½æ¶æå°çå¼ã
	\item æ¯å¦æ¶æå°çå¼ï¼$\Vert\hat{y}^{(iter)}-\hat{\beta_k}^{(iter-1)}\Vert_2^2$ è·ç¦»å°äºä¸å®å¼è®¤ä¸ºè¿­ä»£æ¶æå°çå¼ãæ¨¡ææ°æ®ç¥éçå®åæ°å¯ä»¥ç¨è¿ä¸ªæ¡ä»¶è¿è¡å¤æ­ï¼çå®è®¡ç®ä¸­æ²¡æè¯¥å¤æ­æ¡ä»¶ã
\end{enumerate}

\begin{figure*}
	\centering
	\subfigure{
		\begin{minipage}[t]{0.33\linewidth}
			\centering
			\includegraphics[width=2.7in]{img/test1-rhosame1.png}\\
			\vspace{0.02cm}
			\includegraphics[width=2.7in]{img/test1-rhodiff1.png}\\
			\vspace{0.02cm}
		\end{minipage}%
	}%
	\subfigure{
	\begin{minipage}[t]{0.33\linewidth}
		\centering
		\includegraphics[width=2.7in]{img/test1-rhosame2.png}\\
		\vspace{0.02cm}
		\includegraphics[width=2.7in]{img/test1-rhodiff2.png}\\
		\vspace{0.02cm}
	\end{minipage}%
	}%
	\subfigure{
	\begin{minipage}[t]{0.33\linewidth}
		\centering
		\includegraphics[width=2.7in]{img/test1-rhosame3.png}\\
		\vspace{0.02cm}
		\includegraphics[width=2.7in]{img/test1-rhodiff3.png}\\
		\vspace{0.02cm}
	\end{minipage}%
	}%
	\centering
	\caption{ç®åè®¾å®ä¸åç±»å«åå¸éè¿­ä»£è½®æ°ååå¾ï¼æ ·æ¬ 20 ä¸ºä¾ï¼ç¬¬ä¸è¡ä¸ºç¸åç±»å«å±ç¨ $\rho$ çè®¾å®ï¼ç¬¬äºè¡ä¸ºåç±»å«åå«æ´æ°åèª $\rho_k$ çè®¾å®ï¼}
	\vspace{-0.2cm}
	\label{fig:simple-vis}
\end{figure*}




\subsection{Simple Set2-Two Part}

\subsubsection{Estimation}

è¯¥é¨åæ¬è´¨ä¸ä¾æ§æ¯ä¸åå«äº¤äºé¡¹çå¤åç±»åå½é®é¢ï¼å¨ç±»å«åé¨æ

\begin{equation}
	Y_i = X_i^T\beta_k + Z_i^T\alpha_k, if\ i\in Class\ k
\end{equation}

å¶ä¸­ $\beta_k \in \mathbb{R}^p$ï¼$\alpha_k \in \mathbb{R}^q$ï¼è¥è®° $\tilde X_i^T = (X_i^T, Z_i^T), \theta_k^T = (\beta_k^T, \alpha_k^T)$ å¯ä»¥å°æ¨¡åæ¹åä¸º $Y_i = \tilde X_i^T\theta_k$ï¼æ­¤æ¶å®å¨éåä¸ºæç®åçº¿æ§è®¾å®ï¼æ¬é¨åçéç¹å¨äºéªè¯ä¸¤é¨åä¾æ¬¡åèªæ´æ°è¿­ä»£å¬å¼çæ­£ç¡®æ§ã

å¯¹äºç±»å« $k$ï¼å¯ä»¥å¯¹ä¸ååæ°ååºä¼åç®æ ï¼è®¡ç®æ¶ä¾æ¬¡è¿­ä»£å³å¯ã

\paragraph{Estimate $\beta_k$}

\begin{equation}
	\begin{aligned}
		\text{min}\qquad l_k &=  \displaystyle\sum_{i=1}^{n}q_{C_i}(k)\left(y_i - X_i^T \beta_k - Z_i^T \alpha_k\right) \\
		&= (y^{\beta_k} - X\beta_k)^T W_k (y^{\beta_k} - X\beta_k)
	\end{aligned}
\end{equation}

å¶ä¸­ $W_k = diag(q_{C_1}(k),...,q_{C_n}(k))$ï¼å $\beta_k$ çä¼°è®¡å¼ä¸º

\begin{equation}
	\hat \beta_k = (X^{T} W_k X)^{-1} X^{T} W_k y^{\beta_k}
\end{equation}


\paragraph{Estimate $\alpha_k$}

\begin{equation}
	\begin{aligned}
		\text{min}\qquad l_k &=  \displaystyle\sum_{i=1}^{n}q_{C_i}(k)\left(y_i - X_i^T \beta_k - Z_i^T \alpha_k\right) \\
		&= (y^{\alpha_k} - Z\alpha_k)^T W_k (y^{\alpha_k} - Z\alpha_k)
	\end{aligned}
\end{equation}

$\alpha_k$ çä¼°è®¡å¼ä¸º

\begin{equation}
	\hat \alpha_k = (Z^T W_k Z)^{-1}Z^T W_k y^{\alpha_k}
\end{equation}

å¼å¾æ³¨æçæ¯ï¼å°ä¸¤é¨ååæ°å½åä¸ä½çæ¹æ³æ´æ°å¬å¼ \ref{eq:two-part1} ä¸åå«æ´æ° $\beta_k,\alpha_k$ çå¬å¼ \ref{eq:two-part2} å¹¶ä¸ç­ä»·ãå¬å¼ \ref{eq:two-part1} ä¸­ $\beta_k$ çæ´æ°ä¸ä¾èµäºå½å $\alpha_k$ çåå¼ï¼åä¹äº¦åï¼èå¬å¼ \ref{eq:two-part2} ä¸­ $y^{\beta_k} = y - Z\alpha_k,y^{\alpha_k} = y - X\beta_k$ï¼$\beta_k$ ä¸ $\alpha_k$ çæ´æ°ç¸äºä¾èµãç´è§çè§£ä¸æ¥çï¼äºèè®¡ç®ç®æ ä¸åï¼æ¨å¯¼æ¹æ³åç§ååç©éµæ±éã
\begin{equation}
	\left(\begin{array}{c}
		\hat{\beta}_{k} \\
		\hat{\alpha}_k
	\end{array}\right) = \left((X\ Z)^TW_k (X\ Z)\right)^{-1}(X\ Z)^T W_k y
\label{eq:two-part1}
\end{equation}

\begin{equation}
	\left\{\begin{array}{l}
		\hat{\beta}_{k}=  (X^{T} W_k X)^{-1} X^{T} W_k y^{\beta_k}\\
		\hat{\alpha}_{k}= (Z^T W_k Z)^{-1}Z^T W_k y^{\alpha_k}
	\end{array}\right.
	\label{eq:two-part2}
\end{equation}

\subsubsection{Numerical Setting}

ä»¤ $p=4,q=3$ åä¸ªç±»å«çå®åæ°è®¾ç½®è§è¡¨ \ref{tb:coef_true_twopart}

\begin{table}[h]
	\centering
	\caption{ä¸¤é¨ååæ°è®¾å®}
	\begin{tabular}{ccccc}
		\toprule
		& Comp.1 & Comp.2 & Comp.3 & Comp.4 \\
		\midrule
		$\beta_{k1}$  & -2      & -1      & 1      & 2     \\
		$\beta_{k2}$  & -2      & -1      & 1      & 2     \\
		$\beta_{k3}$  & 0      & 0      & 0      & 0      \\
		$\beta_{k4}$  & 0      & 0      & 0      & 0      \\
		$\alpha_{k1}$  & -2      & -1      & 1      & 2      \\
		$\alpha_{k2}$  & -2      & -1      & 1      & 2      \\
		$\alpha_{k3}$  & 0      & 0      & 0      & 0      \\
		\bottomrule
	\end{tabular}
	\label{tb:coef_true_twopart}
\end{table}

\subsubsection{Results}

å¦ææ ¹æ® $Y_i = \tilde X_i^T\theta_k$ ä½¿ç¨æç®æåµæ¹æ³è¿è¡æ±è§£ï¼å¯æ¶æï¼ä½åå§å¼çè®¾ç½®éè¦æ¯è¾å°å¿ãå½åå§å¼ä¸ºä¸ç»æ¥èª $N(0,0.1)$ çéæºæ°æ¶åä¾æ§å¾å¿«ï¼27è½®å·¦å³ï¼æ¶æå°çå¼ï¼ä½è¥æ½æ ·æ­£æåå¸æ¹å·®è¾å¤§ï¼ä¼åºç°åæ°ä¼°è®¡ä¸åååä½æªæ¶æå°çå¼çæåµã

%å¦å¤å¨å®éªä¸­æ³¨æå°ï¼å°ä¸¤é¨ååæ°å½åä¸ä½çè¿­ä»£æ¹æ³ç¸è¾ $\beta_k, \alpha_k$ åå«æ´æ°çæ¹æ³æ¶æè¾æ¢ï¼ä¸ç¡®å®ï¼










\subsection{Simple Set3-Interaction}

\subsubsection{Estimation}

ä»¥ä¸ä¸ºæåç»çç®ååå½æåµï¼ç°èèåæ¨¡åçååéææï¼å³ï¼$X,Z,W$ï¼å¯¹åºåå½åæ°ä¸º $\beta, \alpha, \eta$ï¼å¶ä¸­ $\eta$ åç± $\beta,\gamma$ è®¡ç®å¾å°ï¼çå®éè¦æ´æ°çåæ°ä¸º $\beta, \alpha, \gamma$ï¼ï¼å¶ä¸­ $W$ ä¸ºäº¤äºé¡¹ï¼$Z$ ä¸ºç¯å¢ç¸å³èªåéï¼$X$ ä¸ºåºå ç¸å³èªåéã 
èèå å¥äº¤äºé¡¹ï¼ç®æ å½æ° $\mu_{ik}$ çæååçååï¼å½±åå $\beta,\gamma$ ç¸å³çæ±è§£ãä¸å½±å $\alpha$ çæ±è§£ã

ä¸å¼ \ref{eq:l} å½¢å¼ç±»ä¼¼æ

\begin{equation}
	\begin{aligned}
		l &= \displaystyle\sum_{i=1}^{n}\sum_{k=1}^{K}q_{C_i}(k)\left(C - \frac{\rho^2}{2}(y_i - \mu_{ik})^2 \right) \\
		&= \displaystyle\sum_{i=1}^{n}\sum_{k=1}^{K}q_{C_i}(k)\left(C - \frac{\rho^2}{2}(y_i - X_i^T \beta_k - Z_i^T \alpha_k - W_i^T \eta_k)^2 \right)
	\end{aligned}
\end{equation}

ä¸ºäºæ¹ä¾¿ä»äº¤äºé¡¹ä¸­æç§ä¸åéè¦æååºç³»æ°ï¼éªè¯ä»¥ä¸å ç§è¡¨è¾¾å½¢å¼çç­ä»·ï¼æ°å¼éªè¯ï¼

\begin{equation}
	\begin{aligned}
		W_i^T \eta_i &= W_i^T(I_q\otimes\beta_k \odot \gamma_k) \\
		&= \sum_{s=1}^{q}W_i^{(s)T}\beta_k\odot \gamma_{ks} \\
		&= (\sum_{s=1}^{q}W_i^{(s)}\odot \gamma_{ks})^T \beta_k \\
		&= (W_i \odot (I_q \otimes \beta_k))^T \gamma_k
	\end{aligned}
\label{eq:same}
\end{equation}

å¯¹äºç±»å« $k$ï¼å¯ä»¥å¯¹ä¸ååæ°ååºä¼åç®æ 

\paragraph{Estimate $\alpha_k$}

\begin{equation}
	\begin{aligned}
		\text{min}\qquad l_k &=  \displaystyle\sum_{i=1}^{n}q_{C_i}(k)\left(y_i - X_i^T \beta_k - Z_i^T \alpha_k - W_i^T \eta_k\right) \\
		&= (y^{\alpha_k} - Z\alpha_k)^T W_k (y^{\alpha_k} - Z\alpha_k)
	\end{aligned}
\end{equation}

å¶ä¸­ $y_i^{\alpha_k} = y_i - X_i^T \beta_k - W_i^T \eta_k$, $y^{\alpha_k} = (y_1^{\alpha_k},...,y_n^{\alpha_k})^T$ï¼$W_k = diag(q_{C_1}(k),...,q_{C_n}(k))$ï¼å $\alpha_k$ çä¼°è®¡å¼ä¸º

\begin{equation}
	\hat \alpha_k = (Z^T W_k Z)^{-1}Z^T W_k y^{\alpha_k}
\end{equation}

\paragraph{Estimate $\beta_k$}

\begin{equation}
	\begin{aligned}
		\text{min}\qquad l_k &=  \displaystyle\sum_{i=1}^{n}q_{C_i}(k)\left(y_i - X_i^T \beta_k - Z_i^T \alpha_k - W_i^T \eta_k\right) \\
		&= (y^{\beta_k} - X^{\beta_k}\beta_k)^T W (y^{\beta_k} - X^{\beta_k}\beta_k)
	\end{aligned}
\end{equation}

å¶ä¸­ $X^{\beta_k} = (X^{\beta_k}_1,...,X^{\beta_k}_n)^T \in \mathbb{R}^{n\times p}$, $X^{\beta_k}_i = X_i + (\sum_{s=1}^{q}W_i^{(s)}\odot\gamma_{ks}) \in \mathbb{R}^{p\times 1}$, $y_i^{\beta_k} = y_i - Z_i^T\alpha_k, y_i^{\beta_k} = (y_1^{\beta_k},...,y_n^{\beta_k})^T$ï¼$W_k = diag(q_{C_1}(k),...,q_{C_n}(k))$ï¼å $\beta_k$ çä¼°è®¡å¼ä¸º

\begin{equation}
	\hat \beta_k = (X^{\beta_k T} W_k X^{\beta_k})^{-1} X^{\beta_k T} W_k y^{\beta_k}
\end{equation}

\paragraph{Estimate $\gamma_k$}

\begin{equation}
	\begin{aligned}
		\text{min}\qquad l_k &=  \displaystyle\sum_{i=1}^{n}q_{C_i}(k)\left(y_i - X_i^T \beta_k - Z_i^T \alpha_k - W_i^T \eta_k\right) \\
		&= (y^{\gamma_k} - X^{\gamma_k}\gamma_k)^T W (y^{\gamma_k} -  X^{\gamma_k}\gamma_k)
	\end{aligned}
\end{equation}

å¶ä¸­ $X^{\gamma_k} = (X^{\gamma_k}_1,...,X^{\gamma_k}_n)^T \in \mathbb{R}^{n\times pq}$, $X^{\gamma_k}_i = W_i\odot (I_q \otimes \beta_k) \in \mathbb{R}^{pq\times 1}$, $y_i^{\gamma_k} = y_i - X_i^T\beta_k - Z_i^T\alpha_k, y_i^{\gamma_k} = (y_1^{\gamma_k},...,y_n^{\gamma_k})^T$ï¼$W_k = diag(q_{C_1}(k),...,q_{C_n}(k))$ï¼å $\gamma_k$ çä¼°è®¡å¼ä¸º

\begin{equation}
	\hat \gamma_k = (X^{\gamma_k T} W_k X^{\gamma_k})^{-1} X^{\gamma_k T} W_k y^{\gamma_k}
\end{equation} 

$X^{\beta_k}$ ä¾èµäºå½å $\gamma_k$ ä¼°è®¡å¼ï¼$X^{\gamma_k}$ ä¹ä¾èµäºå½å $\beta_k$ çä¼°è®¡å¼ãæäº¤äºé¡¹çæåµä¸åå¯ä»¥ç´æ¥åä¸ºæç®åççº¿æ§åå½æåµï¼å³ä¸å¯ä»¥ä¸æ¬¡æ§ç®åºä¸è½®åæ°çæ´æ°ãç»¼ä¸ï¼æ´æ°æ»ç»å¦å¼ \ref{eq:interaction}


\begin{equation}
	\left\{\begin{array}{l}
		\hat{\beta}_{k} = (X^{\beta_k T} W_k X)^{-1} X^{\beta_k T} W_k y^{\beta_k}\\
		\hat{\alpha}_{k} = (Z^T W_k Z)^{-1}Z^T W_k y^{\alpha_k}\\
		\hat{\gamma}_{k} = (X^{\gamma_k T} W_k X)^{-1} X^{\gamma_k T} W_k y^{\gamma_k}
	\end{array}\right.
	\label{eq:interaction}
\end{equation}

\subsubsection{Numerical Setting}

1. ä¸»è¦å®éªè®¾ç½®ï¼$p=4,q=3,n=200,K=4$ï¼çå®åæ°è®¾ç½®å¦è¡¨ \ref{tb:coef_true} 

2. è¾å©å®éªè®¾ç½®ï¼$p=2,q=2,n=200,K=4$ï¼çå®åæ°è®¾ç½®å¦è¡¨ \ref{tb:coef_true} ä¸­éé¶é¡¹

\subsubsection{Package Results}

æ³¨æå¨æäº¤äºé¡¹çæ¨¡åä¸­ï¼åå¾å°çç»æåä¸è¿°æ´æ°ä¸åãå½¢å¼ 

\begin{equation}
	\begin{aligned}
		y_i &= \displaystyle\sum_{k=1}^{K} q_{C_i}(k) \left(X_i^T \beta_k + Z_i^T \alpha_k + W_i^T \eta_k\right) \\
		&= \displaystyle\sum_{k=1}^{K} q_{C_i}(k) \left(X_i^T \beta_k + Z_i^T \alpha_k + (W_i \odot (I_q \otimes \beta_k))^T \gamma_k\right)
	\end{aligned}
\end{equation}

å ææå°äºä¹æ³å¾å°çæ¯ $\beta_k, \alpha_k, \gamma_k$ çæ´æ°ï¼$\eta_k$ æ¯æ ¹æ®å¶ä»åæ°è®¡ç®åºæ¥çãèç´æ¥ç¨åå¾å°çåæ°ä¸º $\beta_k, \alpha_k, \eta_k$, $\gamma_k$ éè¦åè®¡ç®ãå½ç¶ä¹å¯ä»¥éæ°å®ä¹ $\gamma_k$ ç¸å³çèªåéä¸º $W_i \odot (I_q \otimes \beta_k)$ï¼ä½è¯¥è¡¨è¾¾å¼åå« $\beta_k$ã

\textit{fmrs}ï¼$p=2,q=2$ ææä¸å¥½

\textit{flexmix}ï¼$p=2,q=2$ è®¡ç®æ­£ç¡®

\begin{table}[h]
	\centering
	\begin{tabular}{ccccc}
		\toprule
		& Comp.1 & Comp.2 & Comp.3 & Comp.4 \\
		\midrule
		$\beta_{k1}$   & 2      & 2      & -2     & -2     \\
		$\beta_{k2}$   & 2      & 2      & -2     & -2     \\
		$\beta_{k3}$   & 0      & 0      & 0      & 0      \\
		$\beta_{k4}$   & 0      & 0      & 0      & 0      \\
		\midrule
		$\alpha_{k1}$  & 2      & 2      & -2     & -2     \\
		$\alpha_{k2}$  & 2      & 2      & -2     & -2     \\
		$\alpha_{k3}$  & 0      & 0      & 0      & 0      \\
		\midrule
		$\gamma_{k1}$  & 3      & 1      & -1     & -3     \\
		$\gamma_{k2}$  & 3      & 1      & -1     & -3     \\
		$\gamma_{k3}$  & 3      & 1      & -1     & -3     \\
		$\gamma_{k4}$  & 3      & 1      & -1     & -3     \\
		$\gamma_{k5}$  & 0      & 0      & 0      & 0      \\
		$\gamma_{k6}$  & 0      & 0      & 0      & 0      \\
		$\gamma_{k7}$  & 0      & 0      & 0      & 0      \\
		$\gamma_{k8}$  & 0      & 0      & 0      & 0      \\
		$\gamma_{k9}$  & 0      & 0      & 0      & 0      \\
		$\gamma_{k10}$ & 0      & 0      & 0      & 0      \\
		$\gamma_{k11}$ & 0      & 0      & 0      & 0      \\
		$\gamma_{k12}$ & 0      & 0      & 0      & 0     \\
		\bottomrule
	\end{tabular}
	\label{tb:coef_true}
\end{table}

ç´æ¥è®¡ç®æµç¨ç±»ä¼¼äºç®æ³ \ref{alg:simplest}ï¼åºå®å¶ä»åæ°ä¾æ¬¡æ´æ° $\beta_k, \alpha_k, \gamma_k$ å³å¯ã

\subsubsection{Iteration Results}

ç´æ¥è®¡ç®ç®æ³ä¸æ¶æä¸è·ç¦»çå¼è¶æ¥è¶è¿ï¼ä¸ºäºæ£æµé®é¢æå¨ï¼è¿è¡åºå®é¨ååæ°ï¼åªè¿­ä»£å©ä½åæ°çå®éªï¼åæ°è·ç¦»çå¼çè·ç¦»éè¿­ä»£æ­¥æ°ååæåµå¦å¾ \ref{fig:split1} æç¤ºï¼å¾çæ é¢è¡¨ç¤ºç¨äºè¿­ä»£çåæ°ã

\begin{figure*}
	\centering
	\subfigure{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[width=3.5in]{img/split_only_beta.png}\\
			\vspace{0.02cm}
			\includegraphics[width=3.5in]{img/split_only_alpha.png}\\
			\vspace{0.02cm}
			\includegraphics[width=3.5in]{img/split_only_gamma.png}\\
		\end{minipage}%
	}%
		\subfigure{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[width=3.5in]{img/split_only_beta_sc.png}\\
			\vspace{0.02cm}
			\includegraphics[width=3.5in]{img/split_only_alpha_sc.png}\\
			\vspace{0.02cm}
			\includegraphics[width=3.5in]{img/split_only_gamma_sc.png}\\
		\end{minipage}%
	}%
	\centering
	\caption{åå«äº¤äºé¡¹æ¨¡åä¸­æ´æ°åä¸ªåæ°ï¼åæ°è·ç¦»çå¼éè¿­ä»£è½®æ°ååç¤ºæå¾ï¼å­å¾æ é¢è¡¨ç¤ºç¨äºè¿­ä»£çåæ°ï¼}
	\vspace{-0.2cm}
	\label{fig:split1}
\end{figure*}

\begin{figure*}
	\centering
	\subfigure{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[width=3.5in]{img/split_except_beta.png}\\
			\vspace{0.02cm}
			\includegraphics[width=3.5in]{img/split_except_alpha.png}\\
			\vspace{0.02cm}
			\includegraphics[width=3.5in]{img/split_except_gamma.png}\\
		\end{minipage}%
	}%
	\subfigure{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[width=3.5in]{img/split_except_beta_sc.png}\\
			\vspace{0.02cm}
			\includegraphics[width=3.5in]{img/split_except_alpha_sc.png}\\
			\vspace{0.02cm}
			\includegraphics[width=3.5in]{img/split_except_gamma_sc.png}\\
		\end{minipage}%
	}%
	\centering
	\caption{åå«äº¤äºé¡¹æ¨¡åä¸­æ´æ°å¤ä¸ªåæ°ï¼åæ°è·ç¦»çå¼éè¿­ä»£è½®æ°ååç¤ºæå¾ï¼å­å¾æ é¢è¡¨ç¤ºç¨äºè¿­ä»£çåæ°ï¼}
	\vspace{-0.2cm}
	\label{fig:split2}
\end{figure*}


\begin{figure*}
	\centering
	\subfigure{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[width=3.5in]{img/split_all.png}\\
			\vspace{0.02cm}
			\includegraphics[width=3.5in]{img/split_all_add.png}\\
		\end{minipage}%
	}%
	\subfigure{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[width=3.5in]{img/split_all_sc.png}\\
			\vspace{0.02cm}
			\includegraphics[width=3.5in]{img/split_all_add_sc.png}\\
		\end{minipage}%
	}%
	\centering
	\caption{åå«äº¤äºé¡¹æ¨¡åä¸­æ´æ°å¤ä¸ªåæ°ï¼åæ°è·ç¦»çå¼éè¿­ä»£è½®æ°ååç¤ºæå¾ï¼å­å¾æ é¢è¡¨ç¤ºç¨äºè¿­ä»£çåæ°ï¼}
	\vspace{-0.2cm}
	\label{fig:split3}
\end{figure*}

ä»å¾ \ref{fig:split1} ä¸­å¯ä»¥çå°æ´æ°åä¸ªåæ°çæææåµé½ä¼æ¶æï¼åç¬æ´æ° $\alpha_k$ æ¶æä¸æ¶æå°çå¼ï¼åç¬æ´æ° $\beta_k$ æ¶æï¼æªæ¶æå°çå¼ï¼ä½åç±»æ­£ç¡®çæ»ä½ä¸åï¼åç¬æ´æ° $\gamma_k$ æ¶æä¸æ¶æå°çå¼ï¼å¶ä¸çå¼è·ç¦»ä¿æå¨ 20 æ¯å ä¸ºåªèè $\alpha_k, \beta_k$ æ¶åªæä¸¤ç§åç±»ï¼$\gamma_k$ æ è®°çæ´ç»è´çåç±»ï¼ç± $\alpha_k, \beta_k$ ååºçç¬¬ä¸å¤§ç±»çç¬¬ä¸ç¬¬äºå°ç±»ä½ç½®ä¸åºå®ï¼å®éæ¶æå°çå¼ã

ä»å¾ \ref{fig:split2} ä¸­å¯ä»¥çå°æ´æ°ä¸¤ä¸ªåæ°çæææåµé½ä¼æ¶æãåªæ´æ° $\alpha_k, \gamma_k$ æ¶æå°éå¸¸æ¥è¿çå¼çå°æ¹ï¼åç±»å¾åè¾¾å° 1ï¼ååªæ´æ° $\beta_k, \gamma_k$ çæåµæ¶æå°æ¥è¿çå¼çå°æ¹ï¼åç±»å¾åè½ç¶åä¸åè¶å¿ä½å±éäº 0.68ï¼åªæ´æ° $\alpha_k, \beta_k$ çæåµæ¶æææä¸å¥½ï¼ç¨³å®å¨ä¸çå¼ç¸å·®è¾è¿çå°æ¹ï¼åç±»å¾åçº¦ä¸º 0.5ï¼æ´æ°ææåæ°æåµä¸æ¶æï¼åç±»å¾åéè¿­ä»£æ¬¡æ°ææ¯è¾å¥æªçå¨ææ§ã

ä»å¾ \ref{fig:split3} æ´æ°ææåæ°æ¶ï¼ç®æ³ä¸æ¶æï¼åç±»å¾ååè·ç¦»çå®åæ°çè·ç¦»ååé½ææ¯è¾å¥æªçå¨ææ§ãç»æ£éªåç°åéçç©éµç¹å¾å¼æè¾å°çæåµï¼å¦ 0.002ãå¾ \ref{fig:split3} ç¬¬ä¸è¡ä¸ºåéé¡¹ä¸å  $0.1 \times$ å¯¹è§éµç»æï¼ç¬¬äºè¡ä¸ºåéé¡¹å  $0.1 \times$ å¯¹è§éµç»æï¼ç»æä¸ç¸åï¼å¨ä¹ååªæ´æ°é¨ååæ°çå®éªä¸­å å¥è¯¥é¡¹å½±åå¾å°ã


\paragraph{æ´æ°åä¸ªåæ° $\beta_k$ è¯¦ç»ä¿¡æ¯}

åªæ´æ° $\alpha_k$ å®éä¸ºå¾çº¯ç²¹ççº¿æ§åå½é®é¢ï¼å®å¨éåä¸º \ref{subsec:test-set1}ï¼åªæ´æ° $\gamma_k$ ä»¥ååªæ´æ° $\beta_k$ çæåµç»è¿éåäº¤äºé¡¹å½¢å¼ä¸å¯å®å¨éåä¸ºç®åçº¿æ§åå½å½¢å¼ï¼å ä¸º $y^{\beta_k}, y^{\gamma_k}, X^{\beta_k}, X^{\gamma_k}$ å¨ä¸åç±»å«ä¸­ä¸åï¼$y^{\alpha_k}, X^{\alpha_k}$ å¨åä¸ªç±»å«ä¸­åç¸åï¼ä¸åç±»å«çåæ°æ´æ°ä¾é  $q_{C_i}$ å ä»¥åºåï¼ã

å¨åªæ´æ° $\beta_k$ èåºå® $\alpha_k, \gamma_k$ æ¶ï¼æ¨¡ååä¸º \ref{eq:beta_test}

\begin{equation}
	\begin{aligned}
		y_i^{\beta_k} &= X_i^{\beta_k T} \beta_k \\
		y_i - Z_i \alpha_k &= \left(X_i + (\sum_{s=1}^{q}W_i^{(s)}\odot\gamma_{ks}) \right)^T \beta_k
	\end{aligned}
\label{eq:beta_test}
\end{equation} 

åºå® $\alpha_k, \gamma_k$ ä¸ºçå¼ï¼åå¨åä¸ªç±»å«ä¸­ $y_i - Z_i \alpha_k, X_i + (\sum_{s=1}^{q}W_i^{(s)}\odot\gamma_{ks})$ åä¸ºåºå®å¼ï¼åæ ·ä¹æ¯ç®åçº¿æ§åå½å½¢å¼ãä¾æ®ä¹åæ¨å¯¼ï¼$\hat{\beta}_{k} = (X^{\beta_k T} W_k X)^{-1} X^{\beta_k T} W_k y^{\beta_k}$ï¼è¿é $W_k = diag(q_{C_1}(k),...,q_{C_n}(k))$. ä¾ç§ä¸å¼è¿è¡æ´æ°ï¼ä¸æ­¥è¿­ä»£å³æ¶æï¼è½æ²¡ææ¶æå°çå¼ï¼ä½åç±»ç»æè¾ä¸ºåçï¼ä½¿ç¨ä¸åæ¹å·®çæ­£æåå¸éæºæ°å¯¹ $\beta_k$ è¿è¡åå§åç»æå¦å¾ \ref{fig:heat-beta} æç¤ºï¼$\beta_k$ çå¼ä¸ºè¡¨ \ref{tb:coef_true} ä¸­ç¸åºé¨åçéé¶å¼ï¼åç±»å®éä¸ºä¸¤ç±»ã


\begin{figure}
	\centering
	\subfigure[$N(0,1)$åå§å]{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[width=3.5in]{img/q_c_beta.png}
		\end{minipage}%
	}%
	\subfigure[$N(0,4)$åå§å]{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[width=3.5in]{img/q_c_beta2.png}\\
		\end{minipage}%
	}%
	\caption{åç¬æ´æ° $\beta_k$ æåµæ¶æååéªæ¦çåå¸å¾}
	\label{fig:heat-beta}
\end{figure}


\subsubsection{Test Detail}

\paragraph{åç±»æ£æµ}

æµè¯å¦æåªæä¸ä¸ªåç±»ï¼50 ä¸ªæ ·æ¬ï¼2+2+4 ä¸ªå¾è®¡ç®åæ°ï¼ï¼ä¾æ¬¡è¿­ä»£ $\alpha_k, \beta_k, \gamma_k$ æ£æ¥åæ°æ´æ°æµç¨èªèº«æ¯å¦ææè¯¯ãç»è®ºï¼åç¬æ´æ°åæ°æ²¡æé®é¢ï¼æ´æ° $\alpha_k, \beta_k$ æ æ´æ° $\alpha_k, \gamma_k$ æ¶åå æ¬¡è¿­ä»£åæ´æ°å°çå¼ï¼$N(0,1)$ éæºåå§åï¼ãä½¿ç¨ä¸ä¸è½®ç»ææ´æ° $\beta_k, \gamma_k$ æ¶æ æ³æ¶æï¼åæ°è·ç¦»çå¼çè·ç¦»æå¾å¼ºä¸æè§å¾çè·³è·æ§ï¼å¦å¾ \ref{fig:beta-gamma-jump}ï¼ä½è¥å®æ¶ä½¿ç¨ææ° $\beta_k, \gamma_k$ è¿è¡æ´æ°ç®æ³æ¶æï¼ä¸åæ°æ¶æå°çå¼ï¼è¿­ä»£æ¬¡æ°çº¦ä¸º 70 æ¬¡ï¼åæ°è·ç¦»çå¼çè·ç¦»éè¿­ä»£æ¬¡æ°çååå¦å¾ \ref{fig:beta-gamma-latest}ã

\begin{figure}
	\centering
	\subfigure[$N(0,1)$åå§å]{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[width=3.5in]{img/beta_gamma_jump.png}
		\end{minipage}%
	}%
	\subfigure[çå¼+$N(0,1)$åå§å]{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[width=3.5in]{img/beta_gamma_jump2.png}\\
		\end{minipage}%
	}%
	\caption{åç±»æ´æ° $\beta_k, \gamma_k$ åæ°è·ç¦»çå¼è·ç¦»éè¿­ä»£è½®æ°çååï¼ä½¿ç¨ä¸ä¸è½®ç»æï¼}
	\label{fig:beta-gamma-jump}
\end{figure}

\begin{figure}
	\centering
	\subfigure[$N(0,1)$åå§å]{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[width=3.5in]{img/beta_gamma_latest.png}
		\end{minipage}%
	}%
	\caption{åç±»æ´æ° $\beta_k, \gamma_k$ åæ°è·ç¦»çå¼è·ç¦»éè¿­ä»£è½®æ°çååï¼ä½¿ç¨å®æ¶ææ°ç»æï¼}
	\label{fig:beta-gamma-latest}
\end{figure}


ç±æ­¤ç»è®ºå¯åï¼ä¿®æ¹åæ°è¿­ä»£æµç¨åä¸ºä½¿ç¨æ¬è½®ææ°ç»æè¿è¡åç»­æ´æ°ï¼åæ¶æ´æ° $\alpha_k, \beta_k, \gamma_k$ çç»æå¦å¾ \ref{fig:one-class-test-old-n200} æç¤ºã



å¢å¤§ $n=800$ ä½¿å¾æ¯ä¸ªç±»å«ç $n_k = 200$ åä½¿ç¨ç¸åçæ¹æ³è¿è¡ä¸ç»å®éªï¼ç»æå¦ä¸ï¼ä¾æ§æä¸æ¶æçæåµï¼æé¤ n å°çåå ãè¿­ä»£ä¸æ¶ææ¶ï¼åºç°æ±éç©éµçæå°ç¹å¾å¼è¿å°ï¼0.001ï¼çæåµï¼å ä¸ 0.1*åä½éµé¡¹åä¸ªå«ä¸æ¶ææ ·ä¾å¯ä»¥æ¶æï¼ä½å¤§å¤æ°æåµæ æ³æ¹åï¼æé¤åéçç©éµå¥å¼çé®é¢ã

åèæç«  \cite{wu2020structured}ï¼ä½¿ç¨æ°çåå§åæ¹æ³ï¼å³ $\beta_k^{(0)}=0, \gamma_k^{(0)}=0$ï¼$\alpha_k^{(0)} = (Z^TZ)^{-1}Z^Ty^{\alpha_k}$ãæ­¤æ¶ä¸åè®¾è®¡åå§åéæºç­ç¥çé®é¢ï¼å¯¹äºç¸åçä¸ç»æ°æ®åªä¼äº§çç¸åçè¿­ä»£è·¯å¾åç»æãåå«ä½¿ç¨ $n=200,n=800$ ä»¥åä¸¤ç§åå§åç­ç¥ï¼å¾å°å®éªç»æå¦å¾ \ref{fig:one-class-test-old-n200},\ref{fig:one-class-test-ref-n200},\ref{fig:one-class-test-old-n800},\ref{fig:one-class-test-ref-n800} æç¤ºï¼æ ·æ¬é 200 æ 800 å ä¹æ²¡æå½±åï¼ä½åºå®åå§åï¼å³æç® \cite{wu2020structured} ä¸­çåå§åæ¹æ³ï¼æ¯éæºåå§åæ´ç¨³å®ãæ´æ¢å¤ç»éæºç§å­çææ°æ®è¿è¡å®éªï¼éæºåå§åç­ç¥ä¸­åªæçå¼+$N(0,0.01)$ç¨³å®æ¶æï¼å¶ä»ä¸ç§éæºç­ç¥é½ä¼åºç°ä¸æ¶æææ¶æå°éè¯¯åæ°çæåµï¼çå®è®¡ç®æ¶æ æ³è·ååæ°çå¼ï¼æåºéæ©åºå®åå§åç­ç¥ã

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{img/one_class_test_old_n200.png}
	\caption{åç±»æ´æ°åæ°è·ç¦»çå¼è·ç¦»éè¿­ä»£è½®æ°çååï¼éæºåå§åï¼ï¼$n=200$}
	\label{fig:one-class-test-old-n200}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{img/one_class_test_ref_n200.png}
	\caption{åç±»æ´æ°åæ°è·ç¦»çå¼è·ç¦»éè¿­ä»£è½®æ°çååï¼åºå®åå§åï¼ï¼$n=200$}
	\label{fig:one-class-test-ref-n200}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{img/one_class_test_old_n800.png}
	\caption{åç±»æ´æ°åæ°è·ç¦»çå¼è·ç¦»éè¿­ä»£è½®æ°çååï¼éæºåå§åï¼ï¼$n=800$}
	\label{fig:one-class-test-old-n800}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{img/one_class_test_ref_n800.png}
	\caption{åç±»æ´æ°åæ°è·ç¦»çå¼è·ç¦»éè¿­ä»£è½®æ°çååï¼åºå®åå§åï¼ï¼$n=800$}
	\label{fig:one-class-test-ref-n800}
\end{figure}

åºå®åå§åä¹ä¸æ¶æçä¾å­ï¼seed 11,K=4ï¼å¼å¸¸ï¼;seed 10,K=1ï¼seed 111,K=1ï¼seed 321,K=2ï¼æ¶æä½å°éè¯¯å¼ï¼é¤ seed 11,K=4 å¤åºå®åå§ååç¸å¯¹ç¨³å®ã


ä»¥éæºç§å­ 11 ä¸­çç¬¬ 4 ç±»ä¸ºä¾æ¢ç©¶å¼å¸¸åå ï¼ç»è§å¯åç°å¼å¸¸å¸¸ç± $\gamma_k$ çè¿å¤§å¼å¯¼è´ï¼å¾ \ref{fig:theta_res} åæ äºå¯¹ $(X^{\gamma_k T}W_k X^{\gamma_k} + \theta I)$ ä¸­ä½¿ç¨ä¸åç $\theta$ å¯¹åºçè¿­ä»£ç»æã

\begin{figure}
	\centering
	\subfigure[$\theta = 0$]{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[width=3.5in]{img/theta_all_0.png}
		\end{minipage}%
	}%
	\subfigure[$\theta = 0.1$]{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[width=3.5in]{img/theta_all_01.png}
		\end{minipage}%
	}%

\subfigure[$\theta = 1$]{
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=3.5in]{img/theta_all_1.png}
	\end{minipage}%
}%
\subfigure[$\theta_{\gamma} = 1,\theta_{\alpha,\beta} = 0.1$]{
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=3.5in]{img/theta_gamma.png}
	\end{minipage}%
}%
	\caption{ä¸å $\theta$ è¿­ä»£ç»æï¼seed 11ï¼}
	\label{fig:theta_res}
\end{figure}



è§å¯åç°ï¼ç¬¬åç±»ç $X^T W_k X$ ç©éµç¹å¾å¼ç¹ç¹ä¸å¶ä»ä¸ç±»ä¸åï¼$\beta_k$ ç¹å¾å¼æè¿å¤§å¼ï¼2000å·¦å³ï¼è $\gamma_k$ æç¸å¯¹è¾å°ç¹å¾å¼ï¼3å·¦å³ï¼ï¼èå¶ä»ç±»å«è¯¥ç©éµçç¹å¾å¼é½æ¯å ç¾çæ°´å¹³ï¼æµè¯å¶ä»åºç°å¼å¸¸çéæºç§å­ï¼ååç°ç±»ä¼¼ç»è®ºãå½åé®é¢éç¹ï¼å æçº¿æ§æå°äºä¹ç³»æ°è¨èé®é¢


å¶ä»çæ³ï¼èªåéççææºå¶ï¼ä½è®¾ç½®åä¸ºç¬ç«åéä¹ä¼åºç°ç¸ä¼¼é®é¢ã


%æ³¨æï¼æ»ä½¿ç¨å½åææ°çåæ°å¯è½ä¼æ¹ååä¸ªãä¸¤ä¸ªåæ°æ¶æï¼ä½ä¸ä¸ªåæ°ä¸æ¶æä¸å°çå¼çè·ç¦»æææ¾çå¾è¿è·³è·æ§çé®é¢ï¼å¯¹äºæ´æ°åä¸ªåæ° $\beta_k$ ä¸æ¶æå°çå¼æ²¡æå½±åã
\clearpage
\paragraph{ååç±»ç»å®æ ·æ¬çå®å½ç±»}

åä¸ªåç±»ï¼çå®åæ°ä¾æ§ä¸ºè¡¨ \ref{tb:coef_true} ä¸­çéé¶é¡¹ï¼è¥ç»å® $q_{C_i}$ ççå¼ï¼å³å·²ç¥æ¯ä¸ªæ ·æ¬å±äºåªä¸ªåç±»ï¼æ£æ¥ $\alpha_k, \beta_k, \gamma_k$ æ¯å¦è®¡ç®æ­£ç¡®ã

å ä¸º $q_{C_i}$ çåå¼å¨è¿éåªè½æ¯ 0 æ 1ï¼å®éè®¡ç®ç­ä»·äºåæ¶è¿è¡åä¸ªåç±»æ ·æ¬è®¡ç®ï¼å·²éªè¯ï¼ãç±è¡¨ \ref{tb:one-class-all} å¯ç¥ç¸åç®æ³å¯¹äºä¸åæ°æ®çæ¶ææ§æ¯ä¸åçï¼å¨åºå®æå±ç±»å«çååç±»é®é¢ä¸­åæ°çæ¶æè¦æ±åä¸ªç±»å«çåæ°åæ¶æï¼å³è®¾ç½®åæ°åå§å¼ä¸ºçå¼+$N(0,0.01)$ æ¶å¯æ¶æï¼å¶ä»æåµåä¸æ¶æãç®æ³çæ¶ææ§ååå§å¼å½±åå¾å¤§ã











\clearpage

\section{Test Summary}

\begin{enumerate}
	\item åå§ååæ°èéåéªæ¦çï¼EM ç®æ³é¡ºåºä¸å¯åç½®å¦åæ æ³ä¸ååç»æè¿è¡å¯¹æ¯
	\item $\rho$ çæ´æ°æ¹å¼ï¼æç§ç±»å«ååä¸åç±»å«åå¹³åä½æ¯ä¸ªç±»å« $1/\sigma$ æææ²¡æææ¾å·®å«ï¼ä¸ºäºé²æ­¢æç±»å¼å¸¸å½±åæ´ä½ææå½ååç±»å«æèªå·±çæ¹å·®ãèèå°åæ¶å¨æç»åºç¨åºæ¯ä¸­ï¼ç®æ ç±»å«ä¹ä¸ä¼å¤ªå¤ï¼è¿æ ·çè®¾å®åºè¯¥ä¸ä¼å¢å å¾å¤§è®¡ç®å¤æåº¦ï¼è¥æéè¦åè¿è¡è°æ´
\end{enumerate}



\newpage
\bibliographystyle{plain}
\bibliography{ref}

\end{document}